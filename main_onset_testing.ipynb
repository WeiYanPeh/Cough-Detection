{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.functions_model import (get_CNN_model, get_NN_model)\n",
    "from functions.functions_features import (extract_features, extract_features_CNN)\n",
    "from functions.functions_whisper_model import (\n",
    "    SpeechClassificationDataset, \n",
    "    SpeechClassifier, \n",
    "    train, \n",
    "    evaluate\n",
    ")   \n",
    "\n",
    "from functions.functions_onset_postprocessing import (\n",
    "    binary_to_intervals, \n",
    "    intervals_to_binary, \n",
    "    merge_close_intervals, \n",
    "    inverse_intervals,\n",
    "    remove_isolated_detection,\n",
    "    fill_short_gaps,\n",
    "    remove_amplitude_threshold,\n",
    "    mean_filter_same_length,\n",
    "    )\n",
    "\n",
    "from functions.functions_onset_metric import (\n",
    "    overlap_calculations, \n",
    "    evaluate_intervals_event_based,\n",
    "    evaluate_intervals_duration_based,\n",
    "    get_average_metrics,\n",
    "    get_sum_metrics,\n",
    "    interval_intersection,\n",
    "    safe_divide,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns\n",
    "columns_features = [\n",
    "    'mean', 'variance', 'std_dev', 'max_value', 'min_value', 'rms',\n",
    "    'skewness', 'kurtosis', 'median', 'range_val', 'iqr',\n",
    "    'zcr', 'energy', 'rmse', 'entropy',\n",
    "    'spectral_centroid', 'spectral_bandwidth', 'spectral_contrast',\n",
    "    'spectral_flatness', 'spectral_rolloff', 'chroma_stft',\n",
    "    ]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    columns_features.append(f'mfcc_mean_{i}_mean')\n",
    "    columns_features.append(f'mfcc_{i}_std')\n",
    "\n",
    "column_drop = ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'median', 'range_val', 'iqr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Default values\n",
    "#################################################################################\n",
    "# model_name = 'LR' # RF, GB, LR\n",
    "# model_name = 'RF', # GB, LR\n",
    "# model_name = 'Keras_NN'\n",
    "# model_name = 'CNN'\n",
    "model_name = 'Whisper'\n",
    "\n",
    "segment_length = 0.2\n",
    "\n",
    "if_plot = True; if_save = False\n",
    "# if_plot = False; if_save = True\n",
    "\n",
    "list_dataset_name = [\n",
    "    'coswara', \n",
    "    'coughvid', \n",
    "    'esc50', \n",
    "    'fsdkaggle', \n",
    "    'virufy',\n",
    "    ]\n",
    "\n",
    "dataset_str = '_'.join(list_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Parameters\n",
    "#################################################################################\n",
    "if segment_length == 0.3:\n",
    "    overlap = 0.5\n",
    "    threshold_proba = 0.5 # Probability threshold for model prediction\n",
    "    threshold_amplitude_mean = 0.005 # Minimum amplitude before drop off\n",
    "    threshold_isolate = 1\n",
    "    threshold_max_gap = 0.2 # Minimum gap between 2 interval before combination\n",
    "    threshold_overlap = 0.1 # Minimum % is required for overlap to be TP\n",
    "    threshold_mean_filter = 3\n",
    "\n",
    "elif segment_length == 0.2:\n",
    "    overlap = 0.5 #0.5\n",
    "    threshold_proba = 0.5\n",
    "    threshold_amplitude_mean = 0.005 # Minimum amplitude before drop off\n",
    "    threshold_isolate = 1\n",
    "    threshold_max_gap = 0.2 # Minimum gap between 2 interval before combination\n",
    "    threshold_overlap = 0.1 # Minimum % is required for overlap to be TP\n",
    "    threshold_mean_filter = 3\n",
    "\n",
    "elif segment_length == 0.1:\n",
    "    overlap = 0.5\n",
    "    # threshold_proba = 0.001 # Probability threshold for model prediction\n",
    "    threshold_proba = 0.4\n",
    "    threshold_amplitude_mean = 0.005 # Minimum amplitude before drop off\n",
    "    threshold_isolate = 0\n",
    "    threshold_max_gap = 0.2 # Minimum gap between 2 interval before combination\n",
    "    threshold_overlap = 0.1 # Minimum % is required for overlap to be TP\n",
    "    threshold_mean_filter = 3\n",
    "\n",
    "step_size = segment_length * (1 - overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Load Model\n",
    "#################################################################################\n",
    "if model_name in ['LR', 'DT', 'RF', 'SVM', 'KNN', 'NB', 'NN', 'GB']:\n",
    "    #################################################################################\n",
    "    # ML\n",
    "    #################################################################################\n",
    "    path_model_save = f'Results_Onset/Model_Onset/{dataset_str}/{model_name}_{segment_length}s/'\n",
    "    model_filename = f\"{path_model_save}model_1.joblib\"\n",
    "    scaler_filename = f\"{path_model_save}scaler__1.joblib\"\n",
    "    model = joblib.load(model_filename)\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "elif model_name in ['NN']:\n",
    "    ##################################################################################\n",
    "    # NN\n",
    "    ##################################################################################\n",
    "    path_model_save = f'Results_Onset/Model_Onset/{dataset_str}/Keras_NN_{segment_length}s/'\n",
    "    model = get_NN_model(53)\n",
    "    model.load_weights(f'{path_model_save}model_1.h5')\n",
    "    scaler_filename = f\"{path_model_save}scaler__1.joblib\"\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "elif model_name in ['CNN']:\n",
    "    #################################################################################\n",
    "    # CNN\n",
    "    #################################################################################    \n",
    "    dimension_dictionary = {0.1: 5, 0.2: 9, 0.3: 13, 0.5: 22, 0.7: 31, 1: 22}\n",
    "    dim_first = 128\n",
    "    input_shape = (dim_first, dimension_dictionary[segment_length], 1)\n",
    "    model = get_CNN_model(input_shape)\n",
    "    \n",
    "    path_model_save = f'Results_Onset/Model_CNN_Onset/{dataset_str}/{segment_length}s/'\n",
    "    model.load_weights(f'{path_model_save}model_CNN_{segment_length}s_1.h5')\n",
    "    scaler_filename = f\"{path_model_save}scaler_pipeline_CNN_{segment_length}s_1.joblib\"\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "elif model_name in ['Whisper']:\n",
    "    ##################################################################################\n",
    "    # Whisper\n",
    "    ##################################################################################\n",
    "    path_model_save = f'Results_Onset/Model_Whisper_Onset/{dataset_str}/whisper_best_model_{segment_length}s.pt'\n",
    "    model_checkpoint = \"openai/whisper-base\"\n",
    "    processor = WhisperProcessor.from_pretrained(model_checkpoint)\n",
    "    whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "    encoder = whisper_model.encoder  # this is the encoder module\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    num_labels = 2\n",
    "    state_dict = torch.load(path_model_save)\n",
    "    model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Input\n",
    "#################################################################################\n",
    "list_filepath = ['Dataset_Ming/S001_20260108115951_2026-01-08_11-59_2026-01-08_12-12_1_of_1.m4a']\n",
    "\n",
    "label_onset_manual = [\n",
    "    [5, 6],\n",
    "    [13, 14],\n",
    "    [15, 16],\n",
    "    [29, 30],\n",
    "    [38, 39],\n",
    "    [40, 42],\n",
    "    [49, 50],\n",
    "    [63, 64],\n",
    "    [66, 67],\n",
    "    [79, 80],\n",
    "    [82, 83],\n",
    "    [197, 199],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Main\n",
    "#################################################################################\n",
    "def plot_intervals(ax, intervals, y, color, label=None):\n",
    "    for start, end in intervals:\n",
    "        ax.barh(y=y, width=end - start, left=start, height=0.2, color=color, label=label)\n",
    "        \n",
    "# Function to compute intersection of two interval lists\n",
    "def compute_intersection(intervals1, intervals2):\n",
    "    result = []\n",
    "    for start1, end1 in intervals1:\n",
    "        for start2, end2 in intervals2:\n",
    "            start = max(start1, start2)\n",
    "            end = min(end1, end2)\n",
    "            if start < end:  # valid overlap\n",
    "                result.append([start, end])\n",
    "    return result\n",
    "\n",
    "results_all = []\n",
    "for filepath in tqdm(list_filepath):\n",
    "\n",
    "    filename = filepath.split('/')[-1]\n",
    "    print(filename)\n",
    "\n",
    "    # Load data\n",
    "    # (y, sr) = librosa.load(filepath, duration=180) # mono=True\n",
    "    (y, sr) = librosa.load(filepath) # mono=True\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "    # Get time interval\n",
    "    time_intervals = np.arange(0, duration - segment_length + 2*step_size, step_size)\n",
    "\n",
    "    if if_plot == True:\n",
    "        print('Duration:', duration)\n",
    "        # print('Time:', time_intervals)\n",
    "    \n",
    "    segment_samples = int(segment_length * sr)\n",
    "    step = segment_samples * (1 - overlap)\n",
    "\n",
    "    label_onset = [0 for i in range(len(time_intervals))]\n",
    "\n",
    "    label_pred = []\n",
    "    label_pred_proba = []\n",
    "    list_threshold_amplitude_mean = []\n",
    "\n",
    "    #################################################################################\n",
    "    # Looping through segments\n",
    "    #################################################################################\n",
    "    if len(label_onset) != 0:\n",
    "        # for start_sample in np.arange(0, len(y) - step, step):\n",
    "        for start_sample in tqdm(np.arange(0, len(y), step), position=0, leave=True):\n",
    "            start_sample = int(start_sample)  # convert to int index\n",
    "            segment = y[start_sample:start_sample + segment_samples]\n",
    "\n",
    "            if len(segment) < segment_samples:\n",
    "                padding = np.zeros(segment_samples - len(segment))\n",
    "                segment = np.concatenate((segment, padding))\n",
    "\n",
    "            mean = np.mean(np.abs(segment))\n",
    "            \n",
    "            #################################################################################\n",
    "            # Extract Features and predict\n",
    "            #################################################################################\n",
    "            if mean <= threshold_amplitude_mean:\n",
    "                list_threshold_amplitude_mean.append(0)\n",
    "            else:\n",
    "                list_threshold_amplitude_mean.append(1)\n",
    "                \n",
    "            #################################################################################\n",
    "            # Get probabilities\n",
    "            #################################################################################\n",
    "            if mean <= threshold_amplitude_mean:\n",
    "                pred_proba = np.array([1, 0])\n",
    "                pred = 0\n",
    "                \n",
    "            # Whisper\n",
    "            elif model_name == 'Whisper':                    \n",
    "                # Calculate new number of samples\n",
    "                new_length = int(len(segment) * 16000 / 22500)\n",
    "                segment = resample(segment, new_length)\n",
    "                \n",
    "                segment_np = segment.astype(np.float32)\n",
    "                inputs = processor(segment_np, sampling_rate=16000, return_tensors=\"pt\")\n",
    "                input_features = inputs.input_features.to(device)\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    logits = model(input_features)\n",
    "                    probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "                    pred = int(probabilities[1] >= threshold_proba)\n",
    "                    pred_proba = probabilities\n",
    "\n",
    "            # CNN\n",
    "            elif model_name == 'CNN':\n",
    "                result_row = extract_features_CNN(segment, sr, segment_length)\n",
    "                result_row = np.array(result_row[0]).reshape((1, dim_first,  dimension_dictionary[segment_length]))\n",
    "                result_row = result_row[..., np.newaxis]\n",
    "\n",
    "                result_row = np.array(result_row)\n",
    "                result_row = np.nan_to_num(result_row, nan=0)\n",
    "\n",
    "                pred_proba = model.predict(result_row, verbose=0)[0]\n",
    "                pred = np.argmax(pred_proba)\n",
    "                # pred = pred_proba[1]\n",
    "\n",
    "            # ML / NN\n",
    "            elif model_name in ['LR', 'DT', 'RF', 'SVM', 'KNN', 'NB', 'NN', 'GB', 'Keras_NN']:\n",
    "                result_row = extract_features(segment, sr)\n",
    "                result_row = pd.DataFrame([result_row], columns=columns_features)\n",
    "                for col in column_drop:\n",
    "                    if col in result_row.columns:\n",
    "                        result_row = result_row.drop([col], axis=1)\n",
    "                result_row = np.array(result_row)[0]\n",
    "                result_row = np.nan_to_num(result_row, nan=0)\n",
    "                result_row = scaler.transform(result_row.reshape(1, -1))\n",
    "\n",
    "                # NN\n",
    "                if model_name in ['Keras_NN']:\n",
    "                    pred_proba = model.predict(result_row)\n",
    "                    pred = np.argmax(pred_proba, axis=1)\n",
    "\n",
    "                # ML\n",
    "                else:\n",
    "                    pred_proba = model.predict_proba(result_row)\n",
    "                    # label_pred = np.argmax(pred_proba, axis=1)\n",
    "                    pred = (pred_proba[:, 1] >= threshold_proba).astype(int)[0]\n",
    "                    pred_proba = list(pred_proba[0])\n",
    "\n",
    "            label_pred.append(pred)\n",
    "            label_pred_proba.append(pred_proba)\n",
    "\n",
    "        label_pred_proba = np.array(label_pred_proba)\n",
    "        label_pred_proba = label_pred_proba[:, 1]\n",
    "        \n",
    "        # Preprocessing\n",
    "        label_pred_proba_raw = label_pred_proba.copy() # Keep copy of raw signal\n",
    "        # for _ in range(10):\n",
    "        for _ in range(5):\n",
    "            label_pred_proba = mean_filter_same_length(label_pred_proba, threshold_mean_filter) # Smoothen signal\n",
    "        label_pred = (np.array(label_pred_proba) >= threshold_proba).astype(int) # Get binary outputs\n",
    "        \n",
    "        label_pred = remove_amplitude_threshold(label_pred, list_threshold_amplitude_mean) # Drop low ampliutes -> Was commented in OG\n",
    "        label_pred = fill_short_gaps(label_pred, threshold=1) # Fill in short gaps -> Was commented in OG\n",
    "        label_pred = remove_isolated_detection(label_pred, max_length_sequence=threshold_isolate) # Remove isolated detection -> Was commented in OG\n",
    "\n",
    "        # Convert into intervals\n",
    "        # label_onset_interval = binary_to_intervals(label_onset, time_step=segment_length) # Onset so no overlap\n",
    "        label_onset_interval = label_onset_manual.copy()\n",
    "        label_pred_interval = binary_to_intervals(label_pred, time_step=step_size) # Pred so overlap\n",
    "        \n",
    "        label_pred_interval = merge_close_intervals(label_pred_interval, threshold_max_gap) # Merge close intervals\n",
    "        label_cough_interval_intersect = interval_intersection(label_onset_interval, label_pred_interval)\n",
    "        \n",
    "        # Convert back\n",
    "        label_onset = intervals_to_binary(label_onset_interval, len(label_onset), segment_length) # Onset so no overlap\n",
    "        label_pred = intervals_to_binary(label_pred_interval, len(label_pred), step_size) # Pred so overlap\n",
    "\n",
    "        audio_information = {\n",
    "            'filepath': filepath,\n",
    "            'filename': filename,\n",
    "            'label': 1,\n",
    "            'segment_length': segment_length,\n",
    "            'overlap': overlap,\n",
    "            'label_onset': label_onset,\n",
    "            'label_pred': label_pred,\n",
    "            'len': len(label_onset),\n",
    "            'duration': duration,\n",
    "            'threshold_proba': threshold_proba,\n",
    "            'threshold_amplitude_mean': threshold_amplitude_mean,\n",
    "            'threshold_overlap': threshold_overlap,\n",
    "            'threshold_max_gap': threshold_max_gap,\n",
    "            'label_onset_interval': label_onset_interval,\n",
    "            'label_pred_interval': label_pred_interval,\n",
    "        }\n",
    "        \n",
    "        metrics_event = evaluate_intervals_event_based(label_pred_interval, label_onset_interval, duration, overlap_threshold=threshold_overlap)\n",
    "        metrics_duration = evaluate_intervals_duration_based(label_pred_interval, label_onset_interval, duration)\n",
    "        metrices_combined = {**audio_information, **metrics_event, **metrics_duration}\n",
    "\n",
    "        if if_plot == True:\n",
    "            list_print = ['label_onset_interval', 'label_pred_interval', 'SEN_d', 'SPE_d', 'PRE_d', 'F1_d', 'FAR_d', 'PRE_e', 'REC_e', 'F1_e', 'FAR_e']\n",
    "            for key in list_print:\n",
    "                print(f'{key}: {metrices_combined[key]}')\n",
    "\n",
    "        results_all.append(metrices_combined)\n",
    "\n",
    "        if if_plot == True:\n",
    "            # Create a figure with subplots\n",
    "            fig, axs = plt.subplots(3, 1, figsize=(18, 5), sharex=True)\n",
    "            \n",
    "            # Plot waveform\n",
    "            librosa.display.waveshow(y, sr=sr, ax=axs[0])\n",
    "            axs[0].set_title('Audio Waveform')\n",
    "            axs[0].set_xlabel('')\n",
    "            axs[0].set_ylabel('Amplitude')\n",
    "            axs[0].grid(True)\n",
    "            axs[0].minorticks_on()  # Enable minor ticks for finer control\n",
    "    \n",
    "            # Plot waveform\n",
    "            time_intervals =np.linspace(0, duration, len(label_pred_proba), endpoint=False)\n",
    "            axs[1].plot(time_intervals, label_pred_proba, color='green', marker='o')\n",
    "            axs[1].axhline(y=threshold_proba, linestyle=':', color='black')  # dotted horizontal line at y=0.5\n",
    "            axs[1].set_ylim(-0.1, 1.1)\n",
    "            axs[1].set_title('Prediction Probability')\n",
    "            axs[1].set_ylabel('Probability')\n",
    "  \n",
    "            # Plot onset (red), pred (blue), and intersection (purple)\n",
    "            plot_intervals(axs[2], label_onset_interval, y=0.8, color='red', label='True')\n",
    "            plot_intervals(axs[2], label_pred_interval, y=0.5, color='green', label='Predict')\n",
    "            plot_intervals(axs[2], label_cough_interval_intersect, y=0.2, color='black', label='Intersect')\n",
    "\n",
    "            # Add vertical lines every minute (60 seconds)\n",
    "            minute_marks = np.arange(0, duration + 30, 30)\n",
    "            \n",
    "            for t in minute_marks:\n",
    "                axs[2].axvline(x=t, color='gray', linestyle='--', alpha=0.4)\n",
    "            \n",
    "            axs[2].set_title('Prediction vs True Labels')\n",
    "            axs[2].set_ylim(0.05, 0.95)\n",
    "            axs[2].set_yticks([0.2, 0.5, 0.8])\n",
    "            axs[2].set_yticklabels(['Intersect', 'Predict', 'True'])\n",
    "            axs[2].set_xlabel('Time (s)')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "            # Play audio\n",
    "            display(Audio(data=y, rate=sr))\n",
    "\n",
    "if not os.path.exists(f'Results_Onset_Test/Results_Metrics'):\n",
    "    os.makedirs(f'Results_Onset_Test/Results_Metrics')\n",
    "results_all = pd.DataFrame(results_all, columns=metrices_combined.keys())\n",
    "\n",
    "if if_save == True:\n",
    "    results_all.to_csv(f'Results_Onset_Test/Results_Metrics/results_onset_metrics_{segment_length}s_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate\n",
    "for segment_length in [0.2]:\n",
    "    df = pd.read_csv(f'Results_Onset_Test/Results_Metrics/results_onset_metrics_{segment_length}s_{model_name}_Summary_All.csv')\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "combined_df = combined_df[[\n",
    "    'window_length', 'model', 'type', \n",
    "    'PRE_e', 'REC_e', 'F1_e', 'FAR_e', 'FARh_e',\n",
    "    'SEN_d', 'SPE_d', 'PRE_d', 'F1_d', 'FAR_d', 'FARh_d',\n",
    "]]\n",
    "\n",
    "# Display or save the result\n",
    "print(combined_df)\n",
    "combined_df.to_csv(f'Results_Onset_Test/Results_Metrics/results_onset_metrics_{model_name}_Summary_All.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
