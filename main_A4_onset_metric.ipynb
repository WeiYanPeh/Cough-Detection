{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ast\n",
    "import joblib\n",
    "import os\n",
    "import joblib\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.functions_model import (get_CNN_model, get_NN_model)\n",
    "from functions.functions_features import (extract_features, extract_features_CNN)\n",
    "\n",
    "from functions.functions_whisper_model import (\n",
    "    SpeechClassificationDataset, \n",
    "    SpeechClassifier, \n",
    "    train, \n",
    "    evaluate\n",
    ")\n",
    "\n",
    "from functions.functions_onset_postprocessing import (\n",
    "    binary_to_intervals, \n",
    "    intervals_to_binary, \n",
    "    merge_close_intervals, \n",
    "    inverse_intervals,\n",
    "    remove_isolated_detection,\n",
    "    fill_short_gaps,\n",
    "    remove_amplitude_threshold,\n",
    "    mean_filter_same_length,\n",
    "    )\n",
    "\n",
    "from functions.functions_onset_metric import (\n",
    "    overlap_calculations, \n",
    "    evaluate_intervals_event_based,\n",
    "    evaluate_intervals_duration_based,\n",
    "    get_average_metrics,\n",
    "    get_sum_metrics,\n",
    "    interval_intersection,\n",
    "    safe_divide,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns\n",
    "columns_features = [\n",
    "    'mean', 'variance', 'std_dev', 'max_value', 'min_value', 'rms',\n",
    "    'skewness', 'kurtosis', 'median', 'range_val', 'iqr',\n",
    "    'zcr', 'energy', 'rmse', 'entropy',\n",
    "    'spectral_centroid', 'spectral_bandwidth', 'spectral_contrast',\n",
    "    'spectral_flatness', 'spectral_rolloff', 'chroma_stft',\n",
    "    ]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    columns_features.append(f'mfcc_mean_{i}_mean')\n",
    "    columns_features.append(f'mfcc_{i}_std')\n",
    "\n",
    "column_drop = ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'median', 'range_val', 'iqr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Default values\n",
    "#################################################################################\n",
    "list_dataset_name = [\n",
    "    'coswara', \n",
    "    'coughvid', \n",
    "    'esc50', \n",
    "    'fsdkaggle', \n",
    "    'virufy',\n",
    "    ]\n",
    "\n",
    "dataset_str = '_'.join(list_dataset_name)\n",
    "\n",
    "# model_name = 'LR' # RF, GB, LR\n",
    "# model_name = 'RF', # GB, LR\n",
    "# model_name = 'Keras_NN'\n",
    "# model_name = 'CNN'\n",
    "model_name = 'Whisper'\n",
    "\n",
    "if_plot = True; if_save = False\n",
    "# if_plot = False; if_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment_length in [\n",
    "    # 0.1, \n",
    "    0.2, \n",
    "    # 0.3,\n",
    "    # 0.5,\n",
    "    # 0.7,\n",
    "    # 1,\n",
    "    ]:\n",
    "\n",
    "    print(f'Window Length: {segment_length}')\n",
    "    #################################################################################\n",
    "    # Parameters\n",
    "    #################################################################################\n",
    "    if segment_length == 0.3:\n",
    "        overlap = 0.5\n",
    "        threshold_proba = 0.5 # Probability threshold for model prediction\n",
    "        threshold_amplitude_mean = 0.005 # Minimum amplitude before drop off\n",
    "        threshold_isolate = 1\n",
    "        threshold_max_gap = 0.2 # Minimum gap between 2 interval before combination\n",
    "        threshold_overlap = 0.1 # Minimum % is required for overlap to be TP\n",
    "        threshold_mean_filter = 3\n",
    "\n",
    "    elif segment_length == 0.2:\n",
    "        overlap = 0.5\n",
    "        # threshold_proba = 0.2 # Probability threshold for model prediction\n",
    "        threshold_proba = 0.5\n",
    "        threshold_amplitude_mean = 0.005 # Minimum amplitude before drop off\n",
    "        threshold_isolate = 1\n",
    "        threshold_max_gap = 0.2 # Minimum gap between 2 interval before combination\n",
    "        threshold_overlap = 0.1 # Minimum % is required for overlap to be TP\n",
    "        threshold_mean_filter = 3\n",
    "\n",
    "    elif segment_length == 0.1:\n",
    "        overlap = 0.5\n",
    "        # threshold_proba = 0.001 # Probability threshold for model prediction\n",
    "        threshold_proba = 0.4\n",
    "        threshold_amplitude_mean = 0.005 # Minimum amplitude before drop off\n",
    "        threshold_isolate = 0\n",
    "        threshold_max_gap = 0.2 # Minimum gap between 2 interval before combination\n",
    "        threshold_overlap = 0.1 # Minimum % is required for overlap to be TP\n",
    "        threshold_mean_filter = 3\n",
    "    \n",
    "    #################################################################################\n",
    "    # Main\n",
    "    #################################################################################\n",
    "    step_size = segment_length * (1 - overlap)\n",
    "\n",
    "    #################################################################################\n",
    "    # Load Model\n",
    "    #################################################################################\n",
    "    if model_name in ['LR', 'DT', 'RF', 'SVM', 'KNN', 'NB', 'NN', 'GB']:\n",
    "        #################################################################################\n",
    "        # ML\n",
    "        #################################################################################\n",
    "        print(f'ML: {model_name}')\n",
    "        path_model_save = f'Results_Onset/Model_Onset/{dataset_str}/{model_name}_{segment_length}s/'\n",
    "        model_filename = f\"{path_model_save}model_1.joblib\"\n",
    "        scaler_filename = f\"{path_model_save}scaler__1.joblib\"\n",
    "        model = joblib.load(model_filename)\n",
    "        scaler = joblib.load(scaler_filename)\n",
    "    \n",
    "    elif model_name in ['NN']:\n",
    "        ##################################################################################\n",
    "        # NN\n",
    "        ##################################################################################\n",
    "        path_model_save = f'Results_Onset/Model_Onset/{dataset_str}/Keras_NN_{segment_length}s/'\n",
    "        model = get_NN_model(53)\n",
    "        model.load_weights(f'{path_model_save}model_1.h5')\n",
    "        scaler_filename = f\"{path_model_save}scaler__1.joblib\"\n",
    "        scaler = joblib.load(scaler_filename)\n",
    "    \n",
    "    elif model_name in ['CNN']:\n",
    "        #################################################################################\n",
    "        # CNN\n",
    "        #################################################################################    \n",
    "        dimension_dictionary = {0.1: 5, 0.2: 9, 0.3: 13, 0.5: 22, 0.7: 31, 1: 22}\n",
    "        \n",
    "        dim_first = 128\n",
    "        input_shape = (dim_first, dimension_dictionary[segment_length], 1)\n",
    "        model = get_CNN_model(input_shape)\n",
    "        \n",
    "        path_model_save = f'Results_Onset/Model_CNN_Onset/{dataset_str}/{segment_length}s/'\n",
    "        model.load_weights(f'{path_model_save}model_CNN_{segment_length}s_1.h5')\n",
    "        scaler_filename = f\"{path_model_save}scaler_pipeline_CNN_{segment_length}s_1.joblib\"\n",
    "        scaler = joblib.load(scaler_filename)\n",
    "    \n",
    "    elif model_name in ['Whisper']:\n",
    "        ##################################################################################\n",
    "        # Whisper\n",
    "        ##################################################################################\n",
    "        path_model_save = f'Results_Onset/Model_Whisper_Onset/{dataset_str}/whisper_best_model_{segment_length}s.pt'\n",
    "        \n",
    "        model_checkpoint = \"openai/whisper-base\"\n",
    "        processor = WhisperProcessor.from_pretrained(model_checkpoint)\n",
    "        whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "        encoder = whisper_model.encoder  # this is the encoder module\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        num_labels = 2\n",
    "        state_dict = torch.load(path_model_save)\n",
    "        model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    \n",
    "    results_all = []\n",
    "    for dataset_name in list_dataset_name:\n",
    "        print(dataset_name)\n",
    "    \n",
    "        df_all = pd.read_csv(f'Results_Onset/Data_Onset/Annotation/data_summary_{dataset_name}_{segment_length}s_onset_label.csv')\n",
    "        df_all['label_onset'] = df_all['label_onset'].apply(ast.literal_eval)\n",
    "        df_all['label_event'] = df_all['label_event'].apply(ast.literal_eval)\n",
    "\n",
    "        df_all = df_all.sample(frac=1).groupby('label').head(200).reset_index(drop=True)\n",
    "        \n",
    "        df_all = df_all[df_all['label']==0].reset_index(drop=True)\n",
    "        # df_all = df_all[df_all['label']==1].reset_index(drop=True)\n",
    "        \n",
    "        total_len = len(df_all)\n",
    "        if total_len > 1:\n",
    "            total_len = 30\n",
    "    \n",
    "        for i in tqdm(range(total_len)):\n",
    "    \n",
    "            # try:\n",
    "            if True:\n",
    "    \n",
    "                filepath = df_all['filepath'][i] # Audio path\n",
    "                dataset = df_all['dataset'][i] # Dataset name\n",
    "                filename = df_all['filename'][i]\n",
    "                \n",
    "                label = df_all['label'][i]\n",
    "                age = df_all['age'][i]\n",
    "                gender = df_all['gender'][i]\n",
    "                status = df_all['status'][i]\n",
    "                \n",
    "                # print(f'{dataset} {filename} {label}')\n",
    "        \n",
    "                # Load data\n",
    "                (y, sr) = librosa.load(filepath) # mono=True\n",
    "                duration = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "                # Get time interval\n",
    "                time_intervals = np.arange(0, duration - segment_length + 2*step_size, step_size)\n",
    "        \n",
    "                # if if_plot == True:\n",
    "                #     print('Duration:', duration)\n",
    "                #     print('Time:', time_intervals)\n",
    "        \n",
    "                segment_samples = int(segment_length * sr)\n",
    "                step = segment_samples * (1 - overlap)\n",
    "        \n",
    "                try:\n",
    "                    label_onset = df_all['label_onset'][i]\n",
    "                    label_onset = list(label_onset)\n",
    "                except:\n",
    "                    label_onset = [0 for i in range(len(time_intervals))]\n",
    "        \n",
    "                label_pred = []\n",
    "                label_pred_proba = []\n",
    "                list_threshold_amplitude_mean = []\n",
    "        \n",
    "                #################################################################################\n",
    "                # Looping through segments\n",
    "                #################################################################################\n",
    "                if len(label_onset) != 0:\n",
    "                    # for start_sample in np.arange(0, len(y) - step, step):\n",
    "                    for start_sample in np.arange(0, len(y), step):\n",
    "                        start_sample = int(start_sample)  # convert to int index\n",
    "                        segment = y[start_sample:start_sample + segment_samples]\n",
    "            \n",
    "                        if len(segment) < segment_samples:\n",
    "                            padding = np.zeros(segment_samples - len(segment))\n",
    "                            segment = np.concatenate((segment, padding))\n",
    "        \n",
    "                        amplitude_mean = np.mean(np.abs(segment))\n",
    "                        \n",
    "                        #################################################################################\n",
    "                        # Extract Features and predict\n",
    "                        #################################################################################\n",
    "                        if amplitude_mean <= threshold_amplitude_mean:\n",
    "                            list_threshold_amplitude_mean.append(0)\n",
    "                        else:\n",
    "                            list_threshold_amplitude_mean.append(1)\n",
    "                            \n",
    "                        #################################################################################\n",
    "                        # Get probabilities\n",
    "                        #################################################################################\n",
    "                        if amplitude_mean <= threshold_amplitude_mean:\n",
    "                            pred_proba = np.array([1, 0])\n",
    "                            pred = 0\n",
    "                            \n",
    "                        # Whisper\n",
    "                        elif model_name == 'Whisper':                    \n",
    "                            # Calculate new number of samples\n",
    "                            new_length = int(len(segment) * 16000 / 22500)\n",
    "                            segment = resample(segment, new_length)\n",
    "                            \n",
    "                            segment_np = segment.astype(np.float32)\n",
    "                            inputs = processor(segment_np, sampling_rate=16000, return_tensors=\"pt\")\n",
    "                            input_features = inputs.input_features.to(device)\n",
    "                        \n",
    "                            with torch.no_grad():\n",
    "                                logits = model(input_features)\n",
    "                                probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "                                # pred = np.argmax(probabilities)\n",
    "                                # if segment_length == 0.1:\n",
    "                                #     probabilities = min(10 * probabilities[1], 1)\n",
    "                                #     probabilities = [1-probabilities, probabilities]\n",
    "                                    \n",
    "                                # elif segment_length == 0.2:\n",
    "                                #     probabilities = min(3 * probabilities[1], 1)\n",
    "                                #     probabilities = [1-probabilities, probabilities]\n",
    "                                \n",
    "                                # pred = (probabilities[1] >= threshold_proba).astype(int)\n",
    "                                pred = int(probabilities[1] >= threshold_proba)\n",
    "                                pred_proba = probabilities\n",
    "        \n",
    "                        # CNN\n",
    "                        elif model_name == 'CNN':\n",
    "                            result_row = extract_features_CNN(segment, sr, segment_length)\n",
    "                            result_row = np.array(result_row[0]).reshape((1, dim_first,  dimension_dictionary[segment_length]))\n",
    "                            result_row = result_row[..., np.newaxis]\n",
    "        \n",
    "                            result_row = np.array(result_row)\n",
    "                            result_row = np.nan_to_num(result_row, nan=0)\n",
    "        \n",
    "                            pred_proba = model.predict(result_row, verbose=0)[0]\n",
    "                            pred = np.argmax(pred_proba)\n",
    "                            # pred = pred_proba[1]\n",
    "        \n",
    "                        # ML / NN\n",
    "                        elif model_name in ['LR', 'DT', 'RF', 'SVM', 'KNN', 'NB', 'NN', 'GB', 'Keras_NN']:\n",
    "                            result_row = extract_features(segment, sr)\n",
    "                            result_row = pd.DataFrame([result_row], columns=columns_features)\n",
    "                            for col in column_drop:\n",
    "                                if col in result_row.columns:\n",
    "                                    result_row = result_row.drop([col], axis=1)\n",
    "                            result_row = np.array(result_row)[0]\n",
    "                            result_row = np.nan_to_num(result_row, nan=0)\n",
    "                            result_row = scaler.transform(result_row.reshape(1, -1))\n",
    "        \n",
    "                            # NN\n",
    "                            if model_name in ['Keras_NN']:\n",
    "                                pred_proba = model.predict(result_row)\n",
    "                                pred = np.argmax(pred_proba, axis=1)\n",
    "        \n",
    "                            # ML\n",
    "                            else:\n",
    "                                pred_proba = model.predict_proba(result_row)\n",
    "                                # label_pred = np.argmax(pred_proba, axis=1)\n",
    "                                pred = (pred_proba[:, 1] >= threshold_proba).astype(int)[0]\n",
    "                                pred_proba = list(pred_proba[0])\n",
    "        \n",
    "                        label_pred.append(pred)\n",
    "                        label_pred_proba.append(pred_proba)\n",
    "        \n",
    "                    label_pred_proba = np.array(label_pred_proba)\n",
    "                    label_pred_proba = label_pred_proba[:, 1]\n",
    "                    \n",
    "                    # if if_plot == True:\n",
    "                    #     print('True:', label_onset)\n",
    "                    #     print('Pred:', label_pred)\n",
    "                    \n",
    "                    # Preprocessing\n",
    "                    label_pred_proba_raw = label_pred_proba.copy() # Keep copy of raw signal\n",
    "                    for _ in range(10):\n",
    "                        label_pred_proba = mean_filter_same_length(label_pred_proba, threshold_mean_filter) # Smoothen signal\n",
    "                    label_pred = (np.array(label_pred_proba) >= threshold_proba).astype(int) # Get binary outputs\n",
    "                    \n",
    "                    # label_pred = remove_amplitude_threshold(label_pred, list_threshold_amplitude_mean) # Drop low ampliutes\n",
    "                    # label_pred = fill_short_gaps(label_pred, threshold=1) # Fill in short gaps\n",
    "                    # label_pred = remove_isolated_detection(label_pred, max_length_sequence=threshold_isolate) # Remove isolated detection\n",
    "        \n",
    "                    # Convert into intervals\n",
    "                    label_onset_interval = binary_to_intervals(label_onset, time_step=segment_length) # Onset so no overlap\n",
    "                    label_pred_interval = binary_to_intervals(label_pred, time_step=step_size) # Pred so overlap\n",
    "                    \n",
    "                    label_pred_interval = merge_close_intervals(label_pred_interval, threshold_max_gap) # Merge close intervals\n",
    "                    label_cough_interval_intersect = interval_intersection(label_onset_interval, label_pred_interval)\n",
    "                    \n",
    "                    # Convert back\n",
    "                    label_onset = intervals_to_binary(label_onset_interval, len(label_onset), segment_length) # Onset so no overlap\n",
    "                    label_pred = intervals_to_binary(label_pred_interval, len(label_pred), step_size) # Pred so overlap\n",
    "        \n",
    "                    audio_information = {\n",
    "                        'filepath': filepath,\n",
    "                        'dataset': dataset,\n",
    "                        'filename': filename,\n",
    "                        'label': label,\n",
    "                        'age': age,\n",
    "                        'gender': gender,\n",
    "                        'status': status,\n",
    "                        'segment_length': segment_length,\n",
    "                        'overlap': overlap,\n",
    "                        'label_onset': label_onset,\n",
    "                        'label_pred': label_pred,\n",
    "                        'len': len(label_onset),\n",
    "                        'duration': duration,\n",
    "                        'threshold_proba': threshold_proba,\n",
    "                        'threshold_amplitude_mean': threshold_amplitude_mean,\n",
    "                        'threshold_overlap': threshold_overlap,\n",
    "                        'threshold_max_gap': threshold_max_gap,\n",
    "                        \n",
    "                        'label_onset_interval': label_onset_interval,\n",
    "                        'label_pred_interval': label_pred_interval,\n",
    "                    }\n",
    "                    \n",
    "                    metrics_event = evaluate_intervals_event_based(\n",
    "                        label_pred_interval, \n",
    "                        label_onset_interval, \n",
    "                        duration, \n",
    "                        overlap_threshold=threshold_overlap)\n",
    "    \n",
    "                    metrics_duration = evaluate_intervals_duration_based(\n",
    "                        label_pred_interval, \n",
    "                        label_onset_interval, \n",
    "                        duration)\n",
    "            \n",
    "                    metrices_combined = {**audio_information, **metrics_event, **metrics_duration}\n",
    "    \n",
    "                    if if_plot == True:\n",
    "                        list_print = [\n",
    "                            'label_onset_interval', 'label_pred_interval',\n",
    "                            # 'total_cough_intersect_duration', \n",
    "                            # 'total_non_cough_intersect_duration',\n",
    "                            # 'total_cough_duration', 'total_non_cough_duration', 'total_pred_duration',\n",
    "                            'SEN_d', 'SPE_d', 'PRE_d', 'F1_d', 'FAR_d',\n",
    "\n",
    "                            'PRE_e', 'REC_e', 'F1_e', 'FAR_e',\n",
    "                            ]\n",
    "    \n",
    "                        for key in list_print:\n",
    "                            print(f'{key}: {metrices_combined[key]}')\n",
    "        \n",
    "                    results_all.append(metrices_combined)\n",
    "        \n",
    "                    if if_plot == True:\n",
    "                        # Create a figure with subplots\n",
    "                        fig, axs = plt.subplots(3, 1, figsize=(6, 5), \n",
    "                                                sharex=True\n",
    "                                               )\n",
    "                        \n",
    "                        # Plot waveform\n",
    "                        librosa.display.waveshow(y, sr=sr, ax=axs[0])\n",
    "                        axs[0].set_title('Audio Waveform')\n",
    "                        axs[0].set_xlabel('')\n",
    "                        axs[0].set_ylabel('Amplitude')\n",
    "                        axs[0].grid(True)\n",
    "                        axs[0].minorticks_on()  # Enable minor ticks for finer control\n",
    "                \n",
    "                        # Plot waveform\n",
    "                        time_intervals =np.linspace(0, duration, len(label_pred_proba), endpoint=False)\n",
    "                        # axs[1].plot(time_intervals, label_pred_proba, color='green', marker='o', label='Postprocessed', alpha=0.7)\n",
    "                        # axs[1].plot(time_intervals, label_pred_proba_raw, color='grey', marker='x', label='Raw', alpha=0.7)\n",
    "                        axs[1].plot(time_intervals, label_pred_proba, color='green', marker='o')\n",
    "                        axs[1].axhline(y=threshold_proba, linestyle=':', color='black')  # dotted horizontal line at y=0.5\n",
    "                        axs[1].set_ylim(-0.1, 1.1)\n",
    "                        axs[1].set_title('Prediction Probability')\n",
    "                        axs[1].set_ylabel('Probability')\n",
    "                        # axs[1].legend(loc='upper right', frameon=True)\n",
    "                        \n",
    "                        \n",
    "            \n",
    "                        def plot_intervals(ax, intervals, y, color, label=None):\n",
    "                            for start, end in intervals:\n",
    "                                ax.barh(y=y, width=end - start, left=start, height=0.2, color=color, label=label)\n",
    "                                \n",
    "                        # Function to compute intersection of two interval lists\n",
    "                        def compute_intersection(intervals1, intervals2):\n",
    "                            result = []\n",
    "                            for start1, end1 in intervals1:\n",
    "                                for start2, end2 in intervals2:\n",
    "                                    start = max(start1, start2)\n",
    "                                    end = min(end1, end2)\n",
    "                                    if start < end:  # valid overlap\n",
    "                                        result.append([start, end])\n",
    "                            return result\n",
    "                            \n",
    "                        # Plot onset (red), pred (blue), and intersection (purple)\n",
    "                        plot_intervals(axs[2], label_onset_interval, y=0.8, color='red', label='True')\n",
    "                        plot_intervals(axs[2], label_pred_interval, y=0.5, color='green', label='Predict')\n",
    "                        plot_intervals(axs[2], label_cough_interval_intersect, y=0.2, color='black', label='Intersect')\n",
    "            \n",
    "                        axs[2].set_title('Prediction vs True Labels')\n",
    "                        axs[2].set_ylim(0.05, 0.95)\n",
    "                        axs[2].set_yticks([0.2, 0.5, 0.8])\n",
    "                        axs[2].set_yticklabels(['Intersect', 'Predict', 'True'])\n",
    "                        axs[2].set_xlabel('Time (s)')\n",
    "            \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                \n",
    "                        # Play audio\n",
    "                        display(Audio(data=y, rate=sr))\n",
    "            # except:\n",
    "            #     pass\n",
    "\n",
    "\n",
    "    if not os.path.exists(f'Results_Onset/Results_Metrics'):\n",
    "        os.makedirs(f'Results_Onset/Results_Metrics')\n",
    "    \n",
    "    results_all = pd.DataFrame(results_all, columns=metrices_combined.keys())\n",
    "\n",
    "    if if_save == True:\n",
    "        results_all.to_csv(f'Results_Onset/Results_Metrics/results_onset_metrics_{segment_length}s_{model_name}.csv', index=False)\n",
    "    \n",
    "    #################################################################################\n",
    "    # Get Avg\n",
    "    #################################################################################\n",
    "    results_dict_avg = get_average_metrics(results_all, model_name, segment_length)\n",
    "    # print('Average Metrics')\n",
    "    # for key in results_dict_avg.keys():\n",
    "    #     print(f'{key}: {results_dict_avg[key]}')\n",
    "    \n",
    "    #################################################################################\n",
    "    # Get Sum\n",
    "    #################################################################################\n",
    "    results_dict_sum = get_sum_metrics(results_all, model_name, segment_length)\n",
    "    # print('Sum Metrics')\n",
    "    # for key in results_dict_sum.keys():\n",
    "    #     print(f'{key}: {results_dict_sum[key]}')\n",
    "    \n",
    "    #################################################################################\n",
    "    # Save final results\n",
    "    #################################################################################\n",
    "    results_dict_all = pd.DataFrame([results_dict_avg, results_dict_sum], columns=results_dict_avg.keys())\n",
    "    \n",
    "    results_dict_all = results_dict_all[[\n",
    "        'window_length', 'model', 'type', \n",
    "        'SEN_d', 'SPE_d', 'PRE_d', 'F1_d', 'FAR_d', 'FARh_d',\n",
    "        'PRE_e', 'REC_e', 'F1_e', 'FAR_e', 'FARh_e'\n",
    "    ]]\n",
    "\n",
    "    if if_save == True:\n",
    "        results_dict_all.to_csv(f'Results_Onset/Results_Metrics/results_onset_metrics_{segment_length}s_{model_name}_Summary_All.csv', index=False)\n",
    "    print(results_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate\n",
    "for segment_length in [0.1, 0.2, 0.3, 0.5, 0.7, 1]:\n",
    "    df = pd.read_csv(f'Results_Onset/Results_Metrics/results_onset_metrics_{segment_length}s_{model_name}_Summary_All.csv')\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "combined_df = combined_df[[\n",
    "    'window_length', 'model', 'type', \n",
    "    'PRE_e', 'REC_e', 'F1_e', 'FAR_e', 'FARh_e',\n",
    "    'SEN_d', 'SPE_d', 'PRE_d', 'F1_d', 'FAR_d', 'FARh_d',\n",
    "]]\n",
    "\n",
    "# Display or save the result\n",
    "print(combined_df)\n",
    "combined_df.to_csv(f'Results_Onset/Results_Metrics/results_onset_metrics_{model_name}_Summary_All.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
