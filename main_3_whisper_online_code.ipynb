{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install accelerate -U\n",
    "# !pip3 install datasets transformers[sentencepiece]\n",
    "# !pip3 install --upgrade torchvision\n",
    "# !pip3 install --upgrade transformers\n",
    "# !pip3 install --upgrade regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import io\n",
    "import datasets\n",
    "\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset, DatasetDict,  Audio\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6770</th>\n",
       "      <td>54898-8-0-2.wav</td>\n",
       "      <td>54898</td>\n",
       "      <td>47.992301</td>\n",
       "      <td>51.992301</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>siren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>172338-9-0-7.wav</td>\n",
       "      <td>172338</td>\n",
       "      <td>91.760480</td>\n",
       "      <td>95.760480</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>street_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8556</th>\n",
       "      <td>95562-4-3-0.wav</td>\n",
       "      <td>95562</td>\n",
       "      <td>8.795241</td>\n",
       "      <td>12.795241</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>drilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870</th>\n",
       "      <td>75490-8-0-2.wav</td>\n",
       "      <td>75490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>siren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>128891-3-0-4.wav</td>\n",
       "      <td>128891</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       slice_file_name    fsID      start        end  salience  fold  classID  \\\n",
       "6770   54898-8-0-2.wav   54898  47.992301  51.992301         2     3        8   \n",
       "3534  172338-9-0-7.wav  172338  91.760480  95.760480         1     4        9   \n",
       "8556   95562-4-3-0.wav   95562   8.795241  12.795241         1     3        4   \n",
       "7870   75490-8-0-2.wav   75490   1.000000   5.000000         1     6        8   \n",
       "1226  128891-3-0-4.wav  128891   2.000000   6.000000         1     6        3   \n",
       "\n",
       "             class  \n",
       "6770         siren  \n",
       "3534  street_music  \n",
       "8556      drilling  \n",
       "7870         siren  \n",
       "1226      dog_bark  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df = pd.read_csv(r\"/home/l083319/Cough_Related/Dataset/urbansound8k/UrbanSound8K.csv\")\n",
    "audio_df = audio_df.sample(n=2000, random_state=42)\n",
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "street_music        251\n",
       "jackhammer          238\n",
       "drilling            235\n",
       "air_conditioner     234\n",
       "dog_bark            224\n",
       "engine_idling       218\n",
       "children_playing    215\n",
       "siren               204\n",
       "car_horn             97\n",
       "gun_shot             84\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_full_paths(parent_directory):\n",
    "    # List to store file paths\n",
    "    audio_file_paths = []\n",
    "\n",
    "    # Iterate through audio folders (assuming they are named fold1, fold2, ..., fold10)\n",
    "    for folder_name in range(1, 11):\n",
    "        folder_path = os.path.join(parent_directory, 'fold{}'.format(folder_name))\n",
    "        # Iterate through files in the current folder and add their paths to the list\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.wav'):  # Assuming your audio files have .wav extension\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                audio_file_paths.append(file_path)\n",
    "\n",
    "    # Create a dictionary to map base name to full_path\n",
    "    file_path_dict = {os.path.basename(path): path for path in audio_file_paths}\n",
    "    return file_path_dict\n",
    "\n",
    "audio_files_directory = r\"/home/l083319/Cough_Related/Dataset/urbansound8k\"\n",
    "file_path_dict = get_all_full_paths(audio_files_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6770</th>\n",
       "      <td>54898-8-0-2.wav</td>\n",
       "      <td>54898</td>\n",
       "      <td>47.992301</td>\n",
       "      <td>51.992301</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>siren</td>\n",
       "      <td>/home/l083319/Cough_Related/Dataset/urbansound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>172338-9-0-7.wav</td>\n",
       "      <td>172338</td>\n",
       "      <td>91.760480</td>\n",
       "      <td>95.760480</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>street_music</td>\n",
       "      <td>/home/l083319/Cough_Related/Dataset/urbansound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8556</th>\n",
       "      <td>95562-4-3-0.wav</td>\n",
       "      <td>95562</td>\n",
       "      <td>8.795241</td>\n",
       "      <td>12.795241</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>drilling</td>\n",
       "      <td>/home/l083319/Cough_Related/Dataset/urbansound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870</th>\n",
       "      <td>75490-8-0-2.wav</td>\n",
       "      <td>75490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>siren</td>\n",
       "      <td>/home/l083319/Cough_Related/Dataset/urbansound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>128891-3-0-4.wav</td>\n",
       "      <td>128891</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>/home/l083319/Cough_Related/Dataset/urbansound...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       slice_file_name    fsID      start        end  salience  fold  classID  \\\n",
       "6770   54898-8-0-2.wav   54898  47.992301  51.992301         2     3        8   \n",
       "3534  172338-9-0-7.wav  172338  91.760480  95.760480         1     4        9   \n",
       "8556   95562-4-3-0.wav   95562   8.795241  12.795241         1     3        4   \n",
       "7870   75490-8-0-2.wav   75490   1.000000   5.000000         1     6        8   \n",
       "1226  128891-3-0-4.wav  128891   2.000000   6.000000         1     6        3   \n",
       "\n",
       "             class                                          full_path  \n",
       "6770         siren  /home/l083319/Cough_Related/Dataset/urbansound...  \n",
       "3534  street_music  /home/l083319/Cough_Related/Dataset/urbansound...  \n",
       "8556      drilling  /home/l083319/Cough_Related/Dataset/urbansound...  \n",
       "7870         siren  /home/l083319/Cough_Related/Dataset/urbansound...  \n",
       "1226      dog_bark  /home/l083319/Cough_Related/Dataset/urbansound...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_single_full_path(slice_file_name):\n",
    "    return file_path_dict.get(slice_file_name)\n",
    "\n",
    "# Add 'full_path' column to the DataFrame\n",
    "audio_df['full_path'] = audio_df['slice_file_name'].apply(get_single_full_path)\n",
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(audio_df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1400\n",
      "Val  : 300\n",
      "Test : 300\n"
     ]
    }
   ],
   "source": [
    "print('Train:', len(train_df))\n",
    "print('Val  :', len(val_df))\n",
    "print('Test :', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_dataset = datasets.Dataset.from_dict({\n",
    "    \"audio\": train_df[\"full_path\"].tolist(),\n",
    "    \"labels\": train_df[\"classID\"].tolist()    \n",
    "    }).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "test_audio_dataset = datasets.Dataset.from_dict({\n",
    "    \"audio\": test_df[\"full_path\"].tolist(),\n",
    "    \"labels\": test_df[\"classID\"].tolist()\n",
    "    }).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "val_audio_dataset = datasets.Dataset.from_dict({\n",
    "    \"audio\": val_df[\"full_path\"].tolist(),\n",
    "    \"labels\": val_df[\"classID\"].tolist()\n",
    "    }).cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d456f4bfa9d247e8a09b0afe0ae618ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e563f284e54b50b30875ae70fe37b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe4e2d5ed4a4a089147091c5857ba41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l083319/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_data,  text_processor):\n",
    "        self.audio_data = audio_data\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        inputs = self.text_processor(\n",
    "            self.audio_data[index][\"audio\"][\"array\"],\n",
    "            return_tensors=\"pt\",\n",
    "            sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"]\n",
    "        )\n",
    "\n",
    "        input_features = inputs.input_features\n",
    "        decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
    "\n",
    "        labels = np.array(self.audio_data[index]['labels'])\n",
    "\n",
    "        return input_features, decoder_input_ids, torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpeechClassificationDataset(train_audio_dataset, feature_extractor)\n",
    "test_dataset = SpeechClassificationDataset(test_audio_dataset, feature_extractor)\n",
    "val_dataset = SpeechClassificationDataset(val_audio_dataset, feature_extractor)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, encoder):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
    "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l083319/.local/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_labels = 10\n",
    "\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            input_features, decoder_input_ids, labels = batch\n",
    "\n",
    "            input_features = input_features.squeeze()\n",
    "            input_features = input_features.to(device)\n",
    "\n",
    "            decoder_input_ids = decoder_input_ids.squeeze()\n",
    "            decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(input_features, decoder_input_ids)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 8 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
    "                train_loss = 0.0\n",
    "\n",
    "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), '/home/l083319/Cough_Related/Dataset/urbansound8k/best_model.pt')\n",
    "\n",
    "        print(\"========================================================================================\")\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
    "        print(\"========================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader,  device):\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "\n",
    "            input_features, decoder_input_ids, labels = batch\n",
    "\n",
    "            input_features = input_features.squeeze()\n",
    "            input_features = input_features.to(device)\n",
    "\n",
    "            decoder_input_ids = decoder_input_ids.squeeze()\n",
    "            decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(input_features, decoder_input_ids)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return loss, accuracy, f1, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/175, Train Loss: 2.3303\n",
      "Epoch 1/5, Batch 16/175, Train Loss: 2.1506\n",
      "Epoch 1/5, Batch 24/175, Train Loss: 1.9492\n",
      "Epoch 1/5, Batch 32/175, Train Loss: 1.9016\n",
      "Epoch 1/5, Batch 40/175, Train Loss: 1.7862\n",
      "Epoch 1/5, Batch 48/175, Train Loss: 0.9539\n",
      "Epoch 1/5, Batch 56/175, Train Loss: 1.5466\n",
      "Epoch 1/5, Batch 64/175, Train Loss: 1.2771\n",
      "Epoch 1/5, Batch 72/175, Train Loss: 1.7923\n",
      "Epoch 1/5, Batch 80/175, Train Loss: 0.7236\n",
      "Epoch 1/5, Batch 88/175, Train Loss: 0.8296\n",
      "Epoch 1/5, Batch 96/175, Train Loss: 1.7375\n",
      "Epoch 1/5, Batch 104/175, Train Loss: 1.3535\n",
      "Epoch 1/5, Batch 112/175, Train Loss: 0.5792\n",
      "Epoch 1/5, Batch 120/175, Train Loss: 1.0513\n",
      "Epoch 1/5, Batch 128/175, Train Loss: 0.9317\n",
      "Epoch 1/5, Batch 136/175, Train Loss: 0.6085\n",
      "Epoch 1/5, Batch 144/175, Train Loss: 0.7902\n",
      "Epoch 1/5, Batch 152/175, Train Loss: 0.7334\n",
      "Epoch 1/5, Batch 160/175, Train Loss: 1.4406\n",
      "Epoch 1/5, Batch 168/175, Train Loss: 0.4313\n",
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 0.9922, Val Accuracy: 0.6433, Val F1: 0.6445, Best Accuracy: 0.6433\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/175, Train Loss: 0.8576\n",
      "Epoch 2/5, Batch 16/175, Train Loss: 1.1097\n",
      "Epoch 2/5, Batch 24/175, Train Loss: 0.4173\n",
      "Epoch 2/5, Batch 32/175, Train Loss: 1.0907\n",
      "Epoch 2/5, Batch 40/175, Train Loss: 0.6451\n",
      "Epoch 2/5, Batch 48/175, Train Loss: 0.5655\n",
      "Epoch 2/5, Batch 56/175, Train Loss: 0.5565\n",
      "Epoch 2/5, Batch 64/175, Train Loss: 0.2308\n",
      "Epoch 2/5, Batch 72/175, Train Loss: 0.5199\n",
      "Epoch 2/5, Batch 80/175, Train Loss: 0.9850\n",
      "Epoch 2/5, Batch 88/175, Train Loss: 1.4281\n",
      "Epoch 2/5, Batch 96/175, Train Loss: 0.8889\n",
      "Epoch 2/5, Batch 104/175, Train Loss: 0.5344\n",
      "Epoch 2/5, Batch 112/175, Train Loss: 0.7274\n",
      "Epoch 2/5, Batch 120/175, Train Loss: 0.5980\n",
      "Epoch 2/5, Batch 128/175, Train Loss: 0.8270\n",
      "Epoch 2/5, Batch 136/175, Train Loss: 1.0918\n",
      "Epoch 2/5, Batch 144/175, Train Loss: 1.1357\n",
      "Epoch 2/5, Batch 152/175, Train Loss: 0.4504\n",
      "Epoch 2/5, Batch 160/175, Train Loss: 0.3975\n",
      "Epoch 2/5, Batch 168/175, Train Loss: 0.4015\n",
      "========================================================================================\n",
      "Epoch 2/5, Val Loss: 0.7922, Val Accuracy: 0.7100, Val F1: 0.7299, Best Accuracy: 0.7100\n",
      "========================================================================================\n",
      "Epoch 3/5, Batch 8/175, Train Loss: 0.3065\n",
      "Epoch 3/5, Batch 16/175, Train Loss: 0.9157\n",
      "Epoch 3/5, Batch 24/175, Train Loss: 0.8673\n",
      "Epoch 3/5, Batch 32/175, Train Loss: 0.1839\n",
      "Epoch 3/5, Batch 40/175, Train Loss: 0.8551\n",
      "Epoch 3/5, Batch 48/175, Train Loss: 0.0325\n",
      "Epoch 3/5, Batch 56/175, Train Loss: 0.5043\n",
      "Epoch 3/5, Batch 64/175, Train Loss: 0.3652\n",
      "Epoch 3/5, Batch 72/175, Train Loss: 0.1611\n",
      "Epoch 3/5, Batch 80/175, Train Loss: 0.1423\n",
      "Epoch 3/5, Batch 88/175, Train Loss: 0.4083\n",
      "Epoch 3/5, Batch 96/175, Train Loss: 0.2431\n",
      "Epoch 3/5, Batch 104/175, Train Loss: 0.3228\n",
      "Epoch 3/5, Batch 112/175, Train Loss: 0.3084\n",
      "Epoch 3/5, Batch 120/175, Train Loss: 1.1497\n",
      "Epoch 3/5, Batch 128/175, Train Loss: 0.0722\n",
      "Epoch 3/5, Batch 136/175, Train Loss: 1.0576\n",
      "Epoch 3/5, Batch 144/175, Train Loss: 0.3374\n",
      "Epoch 3/5, Batch 152/175, Train Loss: 0.5570\n",
      "Epoch 3/5, Batch 160/175, Train Loss: 0.8868\n",
      "Epoch 3/5, Batch 168/175, Train Loss: 0.6679\n",
      "========================================================================================\n",
      "Epoch 3/5, Val Loss: 0.5510, Val Accuracy: 0.8267, Val F1: 0.8299, Best Accuracy: 0.8267\n",
      "========================================================================================\n",
      "Epoch 4/5, Batch 8/175, Train Loss: 0.1334\n",
      "Epoch 4/5, Batch 16/175, Train Loss: 0.3406\n",
      "Epoch 4/5, Batch 24/175, Train Loss: 0.3902\n",
      "Epoch 4/5, Batch 32/175, Train Loss: 0.1488\n",
      "Epoch 4/5, Batch 40/175, Train Loss: 0.2297\n",
      "Epoch 4/5, Batch 48/175, Train Loss: 0.0916\n",
      "Epoch 4/5, Batch 56/175, Train Loss: 0.0891\n",
      "Epoch 4/5, Batch 64/175, Train Loss: 0.4029\n",
      "Epoch 4/5, Batch 72/175, Train Loss: 0.4740\n",
      "Epoch 4/5, Batch 80/175, Train Loss: 0.4400\n",
      "Epoch 4/5, Batch 88/175, Train Loss: 0.0427\n",
      "Epoch 4/5, Batch 96/175, Train Loss: 0.2697\n",
      "Epoch 4/5, Batch 104/175, Train Loss: 0.2798\n",
      "Epoch 4/5, Batch 112/175, Train Loss: 0.6523\n",
      "Epoch 4/5, Batch 120/175, Train Loss: 0.1474\n",
      "Epoch 4/5, Batch 128/175, Train Loss: 0.0422\n",
      "Epoch 4/5, Batch 136/175, Train Loss: 1.1123\n",
      "Epoch 4/5, Batch 144/175, Train Loss: 0.0638\n",
      "Epoch 4/5, Batch 152/175, Train Loss: 0.3318\n",
      "Epoch 4/5, Batch 160/175, Train Loss: 0.1457\n",
      "Epoch 4/5, Batch 168/175, Train Loss: 0.3284\n",
      "========================================================================================\n",
      "Epoch 4/5, Val Loss: 0.5857, Val Accuracy: 0.8033, Val F1: 0.8095, Best Accuracy: 0.8267\n",
      "========================================================================================\n",
      "Epoch 5/5, Batch 8/175, Train Loss: 0.0120\n",
      "Epoch 5/5, Batch 16/175, Train Loss: 0.0494\n",
      "Epoch 5/5, Batch 24/175, Train Loss: 0.0121\n",
      "Epoch 5/5, Batch 32/175, Train Loss: 0.4655\n",
      "Epoch 5/5, Batch 40/175, Train Loss: 0.1476\n",
      "Epoch 5/5, Batch 48/175, Train Loss: 0.4610\n",
      "Epoch 5/5, Batch 56/175, Train Loss: 0.1425\n",
      "Epoch 5/5, Batch 64/175, Train Loss: 0.0193\n",
      "Epoch 5/5, Batch 72/175, Train Loss: 0.2337\n",
      "Epoch 5/5, Batch 80/175, Train Loss: 0.6619\n",
      "Epoch 5/5, Batch 88/175, Train Loss: 0.7047\n",
      "Epoch 5/5, Batch 96/175, Train Loss: 0.0413\n",
      "Epoch 5/5, Batch 104/175, Train Loss: 0.7773\n",
      "Epoch 5/5, Batch 112/175, Train Loss: 0.1811\n",
      "Epoch 5/5, Batch 120/175, Train Loss: 0.2976\n",
      "Epoch 5/5, Batch 128/175, Train Loss: 0.5456\n",
      "Epoch 5/5, Batch 136/175, Train Loss: 0.3427\n",
      "Epoch 5/5, Batch 144/175, Train Loss: 0.0788\n",
      "Epoch 5/5, Batch 152/175, Train Loss: 0.1707\n",
      "Epoch 5/5, Batch 160/175, Train Loss: 0.2201\n",
      "Epoch 5/5, Batch 168/175, Train Loss: 0.1292\n",
      "========================================================================================\n",
      "Epoch 5/5, Val Loss: 0.7789, Val Accuracy: 0.7733, Val F1: 0.7656, Best Accuracy: 0.8267\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-331887693b5e>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('/home/l083319/Cough_Related/Dataset/urbansound8k/best_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.82      0.73        39\n",
      "           1       0.93      0.72      0.81        18\n",
      "           2       0.81      0.76      0.79        34\n",
      "           3       1.00      0.97      0.98        33\n",
      "           4       0.86      0.76      0.81        25\n",
      "           5       0.72      0.78      0.75        27\n",
      "           6       1.00      1.00      1.00        18\n",
      "           7       0.83      0.78      0.81        37\n",
      "           8       0.87      0.90      0.89        30\n",
      "           9       0.79      0.77      0.78        39\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.85      0.83      0.83       300\n",
      "weighted avg       0.83      0.82      0.82       300\n",
      "\n",
      "0.8233333333333334\n"
     ]
    }
   ],
   "source": [
    "# state_dict = torch.load('best_model.pt')\n",
    "state_dict = torch.load('/home/l083319/Cough_Related/Dataset/urbansound8k/best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 10\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
    "\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
