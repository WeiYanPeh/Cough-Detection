{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    scale\n",
    "    )\n",
    "from sklearn.model_selection import ( \n",
    "    KFold,\n",
    "    StratifiedKFold\n",
    "    ) \n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score, \n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    "    )\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from functions_model import get_CNN_model, history_loss_acc, evaluate_matrix, ROC_PR_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>filepath</th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration_segment</th>\n",
       "      <th>sample_frequency</th>\n",
       "      <th>...</th>\n",
       "      <th>2806</th>\n",
       "      <th>2807</th>\n",
       "      <th>2808</th>\n",
       "      <th>2809</th>\n",
       "      <th>2810</th>\n",
       "      <th>2811</th>\n",
       "      <th>2812</th>\n",
       "      <th>2813</th>\n",
       "      <th>2814</th>\n",
       "      <th>2815</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esc50</td>\n",
       "      <td>Dataset/ESC-50-master/ESC-50-master/audio/2-73...</td>\n",
       "      <td>2-73260-A-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>5.978531e-02</td>\n",
       "      <td>2.615242e-02</td>\n",
       "      <td>1.454682e-02</td>\n",
       "      <td>1.047784e-01</td>\n",
       "      <td>1.864351e-01</td>\n",
       "      <td>1.453094e-01</td>\n",
       "      <td>1.713713e-02</td>\n",
       "      <td>3.186968e-02</td>\n",
       "      <td>1.535154e-01</td>\n",
       "      <td>1.656509e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coughvid</td>\n",
       "      <td>Dataset/coughvid/6f0a2344-8d8b-4db5-b4c3-cf569...</td>\n",
       "      <td>6f0a2344-8d8b-4db5-b4c3-cf569ca4f7b9.wav</td>\n",
       "      <td>49.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>8.220000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.798310e-05</td>\n",
       "      <td>8.901093e-06</td>\n",
       "      <td>2.895766e-06</td>\n",
       "      <td>1.318840e-07</td>\n",
       "      <td>4.596973e-08</td>\n",
       "      <td>1.749610e-08</td>\n",
       "      <td>1.339242e-08</td>\n",
       "      <td>1.341929e-06</td>\n",
       "      <td>2.508061e-06</td>\n",
       "      <td>1.409820e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coughvid</td>\n",
       "      <td>Dataset/coughvid/b97e6429-d7e6-45fb-badf-49e52...</td>\n",
       "      <td>b97e6429-d7e6-45fb-badf-49e52d85d030.wav</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.840000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>3.274282e-08</td>\n",
       "      <td>1.811613e-08</td>\n",
       "      <td>7.445976e-08</td>\n",
       "      <td>2.263611e-08</td>\n",
       "      <td>1.163220e-08</td>\n",
       "      <td>1.622580e-08</td>\n",
       "      <td>1.739210e-08</td>\n",
       "      <td>3.316624e-12</td>\n",
       "      <td>1.128431e-37</td>\n",
       "      <td>7.547336e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fsdkaggle</td>\n",
       "      <td>Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...</td>\n",
       "      <td>c66094de</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.860000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>8.689960e-06</td>\n",
       "      <td>1.853306e-05</td>\n",
       "      <td>2.552348e-05</td>\n",
       "      <td>3.542432e-05</td>\n",
       "      <td>1.198441e-04</td>\n",
       "      <td>6.339062e-05</td>\n",
       "      <td>1.266576e-04</td>\n",
       "      <td>9.642727e-05</td>\n",
       "      <td>3.226967e-05</td>\n",
       "      <td>7.529062e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fsdkaggle</td>\n",
       "      <td>Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...</td>\n",
       "      <td>44f9acf9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.780000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.343845e-04</td>\n",
       "      <td>1.364622e-03</td>\n",
       "      <td>7.193934e-04</td>\n",
       "      <td>2.822869e-05</td>\n",
       "      <td>6.511133e-03</td>\n",
       "      <td>1.850729e-04</td>\n",
       "      <td>8.441024e-04</td>\n",
       "      <td>3.963575e-04</td>\n",
       "      <td>5.704202e-05</td>\n",
       "      <td>4.064139e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>coswara</td>\n",
       "      <td>Dataset/Coswara-Data/Extracted_data/20200803/5...</td>\n",
       "      <td>5Gn8GDorBDaV9s4JhzMpppc3I3j1_cough-heavy</td>\n",
       "      <td>45.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>no_resp_illness_exposed</td>\n",
       "      <td>8.106667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>2.095462e-36</td>\n",
       "      <td>1.237004e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>fsdkaggle</td>\n",
       "      <td>Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...</td>\n",
       "      <td>a70a5165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>6.599355e-08</td>\n",
       "      <td>7.011929e-08</td>\n",
       "      <td>3.246572e-07</td>\n",
       "      <td>1.019144e-07</td>\n",
       "      <td>7.102659e-08</td>\n",
       "      <td>5.674248e-08</td>\n",
       "      <td>6.087129e-08</td>\n",
       "      <td>8.604128e-08</td>\n",
       "      <td>9.005176e-08</td>\n",
       "      <td>1.213055e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>coswara</td>\n",
       "      <td>Dataset/Coswara-Data/Extracted_data/20210630/i...</td>\n",
       "      <td>iOx0zPyPiMZjCWXuf3RGu8HKq8k1_cough-shallow</td>\n",
       "      <td>39.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>positive_mild</td>\n",
       "      <td>3.584036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>7.848601e-05</td>\n",
       "      <td>2.000064e-04</td>\n",
       "      <td>1.181125e-36</td>\n",
       "      <td>1.181125e-36</td>\n",
       "      <td>1.181125e-36</td>\n",
       "      <td>1.181125e-36</td>\n",
       "      <td>1.181125e-36</td>\n",
       "      <td>1.181125e-36</td>\n",
       "      <td>1.181125e-36</td>\n",
       "      <td>5.276077e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>coswara</td>\n",
       "      <td>Dataset/Coswara-Data/Extracted_data/20200525/E...</td>\n",
       "      <td>ETcnGsA5gNUC1t3cXJY6KXLvF7W2_cough-heavy</td>\n",
       "      <td>49.0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>healthy</td>\n",
       "      <td>7.616145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.044910e-03</td>\n",
       "      <td>5.864761e-04</td>\n",
       "      <td>6.886056e-04</td>\n",
       "      <td>1.302159e-03</td>\n",
       "      <td>6.904940e-04</td>\n",
       "      <td>4.668747e-04</td>\n",
       "      <td>4.311872e-04</td>\n",
       "      <td>3.169482e-04</td>\n",
       "      <td>9.676027e-04</td>\n",
       "      <td>5.043625e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>fsdkaggle</td>\n",
       "      <td>Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...</td>\n",
       "      <td>a70a5165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221666e-07</td>\n",
       "      <td>4.447282e-08</td>\n",
       "      <td>4.937309e-08</td>\n",
       "      <td>1.011209e-05</td>\n",
       "      <td>1.433425e-05</td>\n",
       "      <td>9.355715e-08</td>\n",
       "      <td>1.217605e-07</td>\n",
       "      <td>5.699966e-08</td>\n",
       "      <td>2.925239e-07</td>\n",
       "      <td>1.625474e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2828 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset                                           filepath  \\\n",
       "0         esc50  Dataset/ESC-50-master/ESC-50-master/audio/2-73...   \n",
       "1      coughvid  Dataset/coughvid/6f0a2344-8d8b-4db5-b4c3-cf569...   \n",
       "2      coughvid  Dataset/coughvid/b97e6429-d7e6-45fb-badf-49e52...   \n",
       "3     fsdkaggle  Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...   \n",
       "4     fsdkaggle  Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...   \n",
       "...         ...                                                ...   \n",
       "1995    coswara  Dataset/Coswara-Data/Extracted_data/20200803/5...   \n",
       "1996  fsdkaggle  Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...   \n",
       "1997    coswara  Dataset/Coswara-Data/Extracted_data/20210630/i...   \n",
       "1998    coswara  Dataset/Coswara-Data/Extracted_data/20200525/E...   \n",
       "1999  fsdkaggle  Dataset/FSDKaggle2018/FSDKaggle2018.audio_trai...   \n",
       "\n",
       "                                        filename   age gender  label  \\\n",
       "0                                   2-73260-A-10   0.0      0      0   \n",
       "1       6f0a2344-8d8b-4db5-b4c3-cf569ca4f7b9.wav  49.0   male      1   \n",
       "2       b97e6429-d7e6-45fb-badf-49e52d85d030.wav   0.0      0      0   \n",
       "3                                       c66094de   0.0      0      0   \n",
       "4                                       44f9acf9   0.0      0      0   \n",
       "...                                          ...   ...    ...    ...   \n",
       "1995    5Gn8GDorBDaV9s4JhzMpppc3I3j1_cough-heavy  45.0   male      1   \n",
       "1996                                    a70a5165   0.0      0      1   \n",
       "1997  iOx0zPyPiMZjCWXuf3RGu8HKq8k1_cough-shallow  39.0   male      1   \n",
       "1998    ETcnGsA5gNUC1t3cXJY6KXLvF7W2_cough-heavy  49.0   male      1   \n",
       "1999                                    a70a5165   0.0      0      1   \n",
       "\n",
       "                       status   duration  duration_segment  sample_frequency  \\\n",
       "0                           0   5.000000               1.0             22050   \n",
       "1                    COVID-19   8.220000               1.0             22050   \n",
       "2                           0   9.840000               1.0             22050   \n",
       "3                           0  16.860000               1.0             22050   \n",
       "4                           0  14.780000               1.0             22050   \n",
       "...                       ...        ...               ...               ...   \n",
       "1995  no_resp_illness_exposed   8.106667               1.0             22050   \n",
       "1996                        0  13.000000               1.0             22050   \n",
       "1997            positive_mild   3.584036               1.0             22050   \n",
       "1998                  healthy   7.616145               1.0             22050   \n",
       "1999                        0  13.000000               1.0             22050   \n",
       "\n",
       "      ...          2806          2807          2808          2809  \\\n",
       "0     ...  5.978531e-02  2.615242e-02  1.454682e-02  1.047784e-01   \n",
       "1     ...  2.798310e-05  8.901093e-06  2.895766e-06  1.318840e-07   \n",
       "2     ...  3.274282e-08  1.811613e-08  7.445976e-08  2.263611e-08   \n",
       "3     ...  8.689960e-06  1.853306e-05  2.552348e-05  3.542432e-05   \n",
       "4     ...  1.343845e-04  1.364622e-03  7.193934e-04  2.822869e-05   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "1995  ...  2.095462e-36  2.095462e-36  2.095462e-36  2.095462e-36   \n",
       "1996  ...  6.599355e-08  7.011929e-08  3.246572e-07  1.019144e-07   \n",
       "1997  ...  7.848601e-05  2.000064e-04  1.181125e-36  1.181125e-36   \n",
       "1998  ...  1.044910e-03  5.864761e-04  6.886056e-04  1.302159e-03   \n",
       "1999  ...  1.221666e-07  4.447282e-08  4.937309e-08  1.011209e-05   \n",
       "\n",
       "              2810          2811          2812          2813          2814  \\\n",
       "0     1.864351e-01  1.453094e-01  1.713713e-02  3.186968e-02  1.535154e-01   \n",
       "1     4.596973e-08  1.749610e-08  1.339242e-08  1.341929e-06  2.508061e-06   \n",
       "2     1.163220e-08  1.622580e-08  1.739210e-08  3.316624e-12  1.128431e-37   \n",
       "3     1.198441e-04  6.339062e-05  1.266576e-04  9.642727e-05  3.226967e-05   \n",
       "4     6.511133e-03  1.850729e-04  8.441024e-04  3.963575e-04  5.704202e-05   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995  2.095462e-36  2.095462e-36  2.095462e-36  2.095462e-36  2.095462e-36   \n",
       "1996  7.102659e-08  5.674248e-08  6.087129e-08  8.604128e-08  9.005176e-08   \n",
       "1997  1.181125e-36  1.181125e-36  1.181125e-36  1.181125e-36  1.181125e-36   \n",
       "1998  6.904940e-04  4.668747e-04  4.311872e-04  3.169482e-04  9.676027e-04   \n",
       "1999  1.433425e-05  9.355715e-08  1.217605e-07  5.699966e-08  2.925239e-07   \n",
       "\n",
       "              2815  \n",
       "0     1.656509e-01  \n",
       "1     1.409820e-03  \n",
       "2     7.547336e-10  \n",
       "3     7.529062e-05  \n",
       "4     4.064139e-03  \n",
       "...            ...  \n",
       "1995  1.237004e-08  \n",
       "1996  1.213055e-07  \n",
       "1997  5.276077e-09  \n",
       "1998  5.043625e-04  \n",
       "1999  1.625474e-06  \n",
       "\n",
       "[2000 rows x 2828 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "coswara, coughvid, esc50, fsdkaggle, virufy\n",
      "Window length: 1\n",
      "############################################################\n",
      "{0: 1000, 1: 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 02:02:32.938352: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7774 - loss: 0.8186\n",
      "Test Accuracy:  0.803\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "Fold 1 - F1: 0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7898 - loss: 0.8215\n",
      "Test Accuracy:  0.812\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - F1: 0.818\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8358 - loss: 0.6482\n",
      "Test Accuracy:  0.837\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - F1: 0.84\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8420 - loss: 0.5910\n",
      "Test Accuracy:  0.832\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - F1: 0.842\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8200 - loss: 0.5743\n",
      "Test Accuracy:  0.853\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - F1: 0.856\n",
      "[[786 214]\n",
      " [131 869]]\n",
      "ROC AUC: 0.890995\n",
      "PR AUC: 0.8599723775075283\n",
      "F1: 0.8344971394959838\n",
      "\n",
      "############################################################\n",
      "coswara, coughvid, esc50, fsdkaggle, virufy\n",
      "Window length: 5\n",
      "############################################################\n",
      "{0: 1000, 1: 1000}\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7703 - loss: 0.7672\n",
      "Test Accuracy:  0.798\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - F1: 0.814\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7691 - loss: 0.7540\n",
      "Test Accuracy:  0.783\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "Fold 2 - F1: 0.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8079 - loss: 0.8955\n",
      "Test Accuracy:  0.812\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - F1: 0.83\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7168 - loss: 0.8738\n",
      "Test Accuracy:  0.748\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "Fold 4 - F1: 0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7997 - loss: 0.8180\n",
      "Test Accuracy:  0.815\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "Fold 5 - F1: 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[686 314]\n",
      " [104 896]]\n",
      "ROC AUC: 0.850365\n",
      "PR AUC: 0.8079333513571563\n",
      "F1: 0.8112633754198469\n",
      "\n",
      "############################################################\n",
      "coswara, coughvid, esc50, fsdkaggle, virufy\n",
      "Window length: 10\n",
      "############################################################\n",
      "{0: 1000, 1: 1000}\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7134 - loss: 0.8478\n",
      "Test Accuracy:  0.765\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "Fold 1 - F1: 0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7794 - loss: 0.6562\n",
      "Test Accuracy:  0.798\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "Fold 2 - F1: 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8233 - loss: 0.8206\n",
      "Test Accuracy:  0.822\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "Fold 3 - F1: 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7146 - loss: 2.4042\n",
      "Test Accuracy:  0.745\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "Fold 4 - F1: 0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7834 - loss: 1.7686\n",
      "Test Accuracy:  0.79\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - F1: 0.805\n",
      "[[709 291]\n",
      " [141 859]]\n",
      "ROC AUC: 0.8339000000000001\n",
      "PR AUC: 0.7842655523290187\n",
      "F1: 0.7994996161800881\n",
      "############################################################\n",
      "DONE\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "list_datasets = [\n",
    "    # ['fsdkaggle'],    # 2% cough Counter({0: 1570, 1: 30})\n",
    "    # # # ['virufy'],       # 100% cough Counter({1: 121})\n",
    "    # ['esc50'],        # 2% cough Counter({0: 1960, 1: 40})\n",
    "    # ['coughvid'],     # 30% cough Counter({1: 19777, 0: 10267})\n",
    "    # ['coswara'],      # 25% cough Counter({0: 18914, 1: 5408})\n",
    "    ['coswara', 'coughvid', 'esc50', 'fsdkaggle', 'virufy'], \n",
    "]\n",
    "\n",
    "overlap = 0\n",
    "\n",
    "for window_length in [1, 5, 10]:\n",
    "\n",
    "    df_results = []\n",
    "    for datasets in list_datasets:\n",
    "        datasets.sort()\n",
    "        print('')\n",
    "        print('#'*60)\n",
    "        print(', '.join(datasets))\n",
    "        print('Window length:', window_length)\n",
    "        print('#'*60)\n",
    "        \n",
    "        dataset_str = '_'.join(datasets)\n",
    "\n",
    "        path_model_save = f'Results/Model_CNN/{dataset_str}/CNN_{window_length}s/'\n",
    "        \n",
    "        if not os.path.exists(path_model_save):\n",
    "            os.makedirs(path_model_save)\n",
    "        \n",
    "        ############################################################\n",
    "        # Load data\n",
    "        ############################################################\n",
    "        df_all_combined = pd.DataFrame()\n",
    "        for dataset in datasets:    \n",
    "            df = pd.read_csv(f'Results/Features_CNN/data_{dataset}_features_{window_length}s_{overlap}.csv')\n",
    "            df_all_combined = pd.concat([df_all_combined, df], axis=0)\n",
    "        df_all_combined = df_all_combined.reset_index(drop=True)\n",
    "        \n",
    "        # df_all_combined = df_all_combined.fillna(df.mean())\n",
    "        df_all_combined = df_all_combined.fillna(0)\n",
    "        \n",
    "        ############################################################\n",
    "        # Get label distribution\n",
    "        ############################################################\n",
    "        df_all_combined = df_all_combined[df_all_combined['mean_amplitude'] > 0.005].reset_index(drop=True)\n",
    "        df_all_combined = df_all_combined.sample(frac=1).groupby('label').head(1000).reset_index(drop=True)\n",
    "        list_labels = df_all_combined['label'].tolist()\n",
    "        count_labels = dict(Counter(list_labels))\n",
    "        pprint(count_labels)\n",
    "    \n",
    "        ############################################################\n",
    "        # Get features and labels\n",
    "        ############################################################\n",
    "        y = df_all_combined['label'].tolist()\n",
    "        X = df_all_combined.drop(columns=['label'])\n",
    "        \n",
    "        ############################################################\n",
    "        # Performance Store\n",
    "        ############################################################\n",
    "        list_cm = 0\n",
    "        list_roc_auc, list_pr_auc = [], []\n",
    "        list_pre, list_rec, list_f1 = [], [], []\n",
    "        list_acc, list_spe, list_sen = [], [], []\n",
    "        \n",
    "        ############################################################\n",
    "        # K-fold Cross Validation model evaluation\n",
    "        ############################################################\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_idx = 1\n",
    "        for train_ids, test_ids in kfold.split(X, y):\n",
    "            \n",
    "            ############################################################\n",
    "            # Get dataset split\n",
    "            ############################################################\n",
    "            df_train = X.loc[train_ids]\n",
    "            y_train = np.array(y)[train_ids]\n",
    "    \n",
    "            df_test = X.loc[test_ids]\n",
    "            y_test = np.array(y)[test_ids]\n",
    "    \n",
    "            ############################################################\n",
    "            # Drop useless columns\n",
    "            ############################################################\n",
    "            def drop_columns(df):\n",
    "                columns = [\n",
    "                    'dataset', 'filename', 'filepath', 'age', 'gender', 'status',\n",
    "                    'duration', 'duration_segment', 'sample_frequency', 'mean_amplitude',\n",
    "                    'segment_shape',\n",
    "                    ]\n",
    "                for col in columns:\n",
    "                    if col in df.columns:\n",
    "                        df = df.drop([col], axis=1)\n",
    "                \n",
    "                df = np.array(df)\n",
    "                return df\n",
    "    \n",
    "            X_train = drop_columns(df_train)\n",
    "            X_test = drop_columns(df_test)\n",
    "\n",
    "\n",
    "            ############################################################\n",
    "            # Scaling\n",
    "            ############################################################\n",
    "            scaler = StandardScaler()\n",
    "    \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "    \n",
    "            # Save the scaler to a file\n",
    "            scaler_filename = f\"{path_model_save}scaler_pipeline_CNN_{fold_idx}.joblib\"\n",
    "            joblib.dump(scaler, scaler_filename)\n",
    "    \n",
    "            ############################################################\n",
    "            # Oversampling (if required)\n",
    "            ############################################################\n",
    "            if datasets in [['fsdkaggle'], ['esc50']]:\n",
    "                try:\n",
    "                    oversample = SMOTE(sampling_strategy=0.5, k_neighbors=5)\n",
    "                    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "                except:\n",
    "                    oversample = SMOTE(sampling_strategy=0.5, k_neighbors=3)\n",
    "                    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "            ############################################################\n",
    "            # Reshaping\n",
    "            ############################################################\n",
    "            dimension_dictionary = {\n",
    "                1: 22,\n",
    "                5: 27,\n",
    "                10: 27,\n",
    "            }\n",
    "\n",
    "            dim_first = 128\n",
    "            input_shape = (dim_first, dimension_dictionary[window_length], 1)\n",
    "            X_train = X_train.reshape((len(X_train), dim_first,  dimension_dictionary[window_length]))\n",
    "            X_test = X_test.reshape((len(X_test), dim_first,  dimension_dictionary[window_length]))\n",
    "    \n",
    "            # Add one more axis for CNN\n",
    "            X_train = X_train[..., np.newaxis]\n",
    "            X_test = X_test[..., np.newaxis]\n",
    "            \n",
    "            ############################################################\n",
    "            # Create Model\n",
    "            ############################################################\n",
    "            model = get_CNN_model(input_shape)\n",
    "            # model.summary()\n",
    "    \n",
    "            batch_size = 16\n",
    "            early_stopping_patience = 10\n",
    "\n",
    "            # Add early stopping\n",
    "            my_callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=path_model_save + 'Checkpoints/model_{epoch:02d}_' + f'CNN_{fold_idx}.keras', \n",
    "                    save_freq='epoch', \n",
    "                    save_best_only=True\n",
    "                    ),\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\", \n",
    "                    patience=early_stopping_patience, \n",
    "                    restore_best_weights=True\n",
    "                    )\n",
    "            ]\n",
    "    \n",
    "            # Fit Model\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=my_callbacks,\n",
    "                validation_split=0.15,\n",
    "                verbose=0,\n",
    "                )\n",
    "    \n",
    "            history_loss_acc(history)\n",
    "    \n",
    "            test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "            print('Test Accuracy: ', round(test_acc, 3))\n",
    "    \n",
    "            predictions = model.predict(X_test)\n",
    "            y_predict = []\n",
    "            for i in range(len(predictions)):\n",
    "                predict = np.argmax(predictions[i])\n",
    "                y_predict.append(predict)\n",
    "                        \n",
    "            ############################################################\n",
    "            # Append predictions to df_test and save\n",
    "            ############################################################\n",
    "            df_test['true'] = y_test\n",
    "            df_test['pred'] = y_predict\n",
    "            \n",
    "            path_results_save = f'Results/Results CNN/{dataset_str}/CNN_{window_length}s/'\n",
    "            if not os.path.exists(path_results_save):\n",
    "                os.makedirs(path_results_save)\n",
    "            \n",
    "            df_test.to_csv(f'{path_results_save}Fold_{fold_idx}.csv', index=False)\n",
    "            \n",
    "            ############################################################\n",
    "            # Get evaluation metrics\n",
    "            ############################################################\n",
    "            acc = accuracy_score(y_test, y_predict)\n",
    "            cm = evaluate_matrix(y_test, y_predict)\n",
    "            roc_auc, pr_auc = ROC_PR_curve(y_test, predictions)\n",
    "            pre = precision_score(y_test, y_predict)\n",
    "            rec = recall_score(y_test, y_predict)\n",
    "            f1 = f1_score(y_test, y_predict)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "            spe = tn / (tn + fp)\n",
    "            sen = rec\n",
    "            \n",
    "            ############################################################\n",
    "            # Append results\n",
    "            ############################################################\n",
    "            list_acc.append(acc)\n",
    "            list_cm = list_cm + cm\n",
    "            list_roc_auc.append(roc_auc)\n",
    "            list_pr_auc.append(pr_auc)\n",
    "            list_pre.append(pre)\n",
    "            list_rec.append(rec)\n",
    "            list_f1.append(f1)\n",
    "            list_spe.append(spe)\n",
    "            list_sen.append(sen)\n",
    "\n",
    "            print(f\"Fold {fold_idx} - F1: {round(f1, 3)}\")\n",
    "            \n",
    "            ############################################################\n",
    "            # Save model\n",
    "            ############################################################\n",
    "            # Serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(f\"{path_model_save}model_{fold_idx}.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            \n",
    "            model.save_weights(f\"{path_model_save}model_{fold_idx}.weights.h5\")\n",
    "            model.save(f\"{path_model_save}model_{fold_idx}.h5\")\n",
    "            # loaded_model = get_model()\n",
    "            # loaded_model.load_weights('Results/Model/kfold.h5')\n",
    "    \n",
    "            ############################################################\n",
    "            # Save results\n",
    "            ############################################################\n",
    "            results = [\n",
    "                ', '.join(datasets), count_labels, \n",
    "                window_length, overlap,\n",
    "                'CNN', fold_idx,\n",
    "                acc, sen, spe, pre, rec, f1, roc_auc, pr_auc, cm]\n",
    "            df_results.append(results)\n",
    "    \n",
    "            # To the next fold\n",
    "            fold_idx = fold_idx + 1\n",
    "            \n",
    "        results = [\n",
    "            ', '.join(datasets),  count_labels,\n",
    "            window_length, overlap,\n",
    "            'CNN', 'Avg',\n",
    "            np.mean(list_acc),\n",
    "            np.mean(list_sen),\n",
    "            np.mean(list_spe),\n",
    "            np.mean(list_pre),\n",
    "            np.mean(list_rec),\n",
    "            np.mean(list_f1),\n",
    "            np.mean(list_roc_auc),\n",
    "            np.mean(list_pr_auc),\n",
    "            list_cm]\n",
    "        df_results.append(results)\n",
    "        \n",
    "        print(list_cm)\n",
    "        print(f'ROC AUC: {np.mean(list_roc_auc)}')\n",
    "        print(f'PR AUC: {np.mean(list_pr_auc)}')\n",
    "        print(f'F1: {np.mean(list_f1)}')\n",
    "    \n",
    "    columns = ['dataset', 'label_count', 'window_length', 'overlap',\n",
    "               'model', 'fold', \n",
    "               'acc', 'sen', 'spe', 'pre', 'rec', 'f1', 'auc', 'auprc', 'cm']    \n",
    "    df_results = pd.DataFrame(df_results, columns = columns)\n",
    "    df_results.to_csv(f'Results/Model_CNN/results_prediction_CNN_{window_length}s_{overlap}.csv', index=False)\n",
    "\n",
    "print('#'*60)\n",
    "print('DONE')\n",
    "print('#'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        dataset         label_count  \\\n",
      "0   coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "1   coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "2   coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "3   coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "4   coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "5   coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "6   coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "7   coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "8   coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "9   coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "10  coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "11  coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "12  coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "13  coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "14  coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "15  coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "16  coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "17  coswara, coughvid, esc50, fsdkaggle, virufy  {1: 1000, 0: 1000}   \n",
      "\n",
      "    window_length  overlap model fold     acc    sen    spe       pre    rec  \\\n",
      "0               1        0   CNN    1  0.8025  0.870  0.735  0.766520  0.870   \n",
      "1               1        0   CNN    2  0.8125  0.845  0.780  0.793427  0.845   \n",
      "2               1        0   CNN    3  0.8375  0.855  0.820  0.826087  0.855   \n",
      "3               1        0   CNN    4  0.8325  0.895  0.770  0.795556  0.895   \n",
      "4               1        0   CNN    5  0.8525  0.880  0.825  0.834123  0.880   \n",
      "5               1        0   CNN  Avg  0.8275  0.869  0.786  0.803143  0.869   \n",
      "6               5        0   CNN    1  0.7975  0.885  0.710  0.753191  0.885   \n",
      "7               5        0   CNN    2  0.7825  0.865  0.700  0.742489  0.865   \n",
      "8               5        0   CNN    3  0.8125  0.915  0.710  0.759336  0.915   \n",
      "9               5        0   CNN    4  0.7475  0.995  0.500  0.665552  0.995   \n",
      "10              5        0   CNN    5  0.8150  0.820  0.810  0.811881  0.820   \n",
      "11              5        0   CNN  Avg  0.7910  0.896  0.686  0.746490  0.896   \n",
      "12             10        0   CNN    1  0.7650  0.970  0.560  0.687943  0.970   \n",
      "13             10        0   CNN    2  0.7975  0.815  0.780  0.787440  0.815   \n",
      "14             10        0   CNN    3  0.8225  0.785  0.860  0.848649  0.785   \n",
      "15             10        0   CNN    4  0.7450  0.860  0.630  0.699187  0.860   \n",
      "16             10        0   CNN    5  0.7900  0.865  0.715  0.752174  0.865   \n",
      "17             10        0   CNN  Avg  0.7840  0.859  0.709  0.755078  0.859   \n",
      "\n",
      "          f1       auc     auprc                       cm  \n",
      "0   0.814988  0.860200  0.830377  [[147  53]\\n [ 26 174]]  \n",
      "1   0.818402  0.858400  0.778355  [[156  44]\\n [ 31 169]]  \n",
      "2   0.840295  0.901400  0.892509  [[164  36]\\n [ 29 171]]  \n",
      "3   0.842353  0.910175  0.876676  [[154  46]\\n [ 21 179]]  \n",
      "4   0.856448  0.924800  0.921945  [[165  35]\\n [ 24 176]]  \n",
      "5   0.834497  0.890995  0.859972  [[786 214]\\n [131 869]]  \n",
      "6   0.813793  0.837450  0.787248  [[142  58]\\n [ 23 177]]  \n",
      "7   0.799076  0.850925  0.827961  [[140  60]\\n [ 27 173]]  \n",
      "8   0.829932  0.846825  0.769571  [[142  58]\\n [ 17 183]]  \n",
      "9   0.797595  0.847175  0.806335  [[100 100]\\n [  1 199]]  \n",
      "10  0.815920  0.869450  0.848552  [[162  38]\\n [ 36 164]]  \n",
      "11  0.811263  0.850365  0.807933  [[686 314]\\n [104 896]]  \n",
      "12  0.804979  0.821950  0.749771  [[112  88]\\n [  6 194]]  \n",
      "13  0.800983  0.860825  0.830810  [[156  44]\\n [ 37 163]]  \n",
      "14  0.815584  0.862175  0.794998  [[172  28]\\n [ 43 157]]  \n",
      "15  0.771300  0.788500  0.750512  [[126  74]\\n [ 28 172]]  \n",
      "16  0.804651  0.836050  0.795238  [[143  57]\\n [ 27 173]]  \n",
      "17  0.799500  0.833900  0.784266  [[709 291]\\n [141 859]]  \n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate\n",
    "for window_length in [1, 5, 10]:\n",
    "    df = pd.read_csv(f'Results/Model_CNN/results_prediction_CNN_{window_length}s_{overlap}.csv')\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Display or save the result\n",
    "print(combined_df)\n",
    "combined_df.to_csv(f'Results/Model_CNN/results_prediction_CNN_All.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
