{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    scale\n",
    "    )\n",
    "from sklearn.model_selection import ( \n",
    "    KFold,\n",
    "    StratifiedKFold\n",
    "    ) \n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score, \n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    "    )\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from functions.functions_model import get_CNN_model, history_loss_acc, evaluate_matrix, ROC_PR_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = [\n",
    "    # ['fsdkaggle'],    # 2% cough Counter({0: 1570, 1: 30})\n",
    "    # # # ['virufy'],       # 100% cough Counter({1: 121})\n",
    "    # ['esc50'],        # 2% cough Counter({0: 1960, 1: 40})\n",
    "    # ['coughvid'],     # 30% cough Counter({1: 19777, 0: 10267})\n",
    "    # ['coswara'],      # 25% cough Counter({0: 18914, 1: 5408})\n",
    "    ['coswara', 'coughvid', 'esc50', 'fsdkaggle', 'virufy'], \n",
    "]\n",
    "\n",
    "overlap = 0\n",
    "\n",
    "for window_length in [1, 5, 10]:\n",
    "\n",
    "    df_results = []\n",
    "    for datasets in list_datasets:\n",
    "        datasets.sort()\n",
    "        print('')\n",
    "        print('#'*60)\n",
    "        print(', '.join(datasets))\n",
    "        print('Window length:', window_length)\n",
    "        print('#'*60)\n",
    "        \n",
    "        dataset_str = '_'.join(datasets)\n",
    "\n",
    "        path_model_save = f'Results/Model_CNN/{dataset_str}/CNN_{window_length}s/'\n",
    "        \n",
    "        if not os.path.exists(path_model_save):\n",
    "            os.makedirs(path_model_save)\n",
    "        \n",
    "        ############################################################\n",
    "        # Load data\n",
    "        ############################################################\n",
    "        df_all_combined = pd.DataFrame()\n",
    "        for dataset in datasets:    \n",
    "            df = pd.read_csv(f'Results/Features_CNN/data_{dataset}_features_{window_length}s_{overlap}.csv')\n",
    "            df_all_combined = pd.concat([df_all_combined, df], axis=0)\n",
    "        df_all_combined = df_all_combined.reset_index(drop=True)\n",
    "        \n",
    "        # df_all_combined = df_all_combined.fillna(df.mean())\n",
    "        df_all_combined = df_all_combined.fillna(0)\n",
    "        \n",
    "        ############################################################\n",
    "        # Get label distribution\n",
    "        ############################################################\n",
    "        df_all_combined = df_all_combined[df_all_combined['mean_amplitude'] > 0.005].reset_index(drop=True)\n",
    "        df_all_combined = df_all_combined.sample(frac=1).groupby('label').head(1000).reset_index(drop=True)\n",
    "        list_labels = df_all_combined['label'].tolist()\n",
    "        count_labels = dict(Counter(list_labels))\n",
    "        pprint(count_labels)\n",
    "    \n",
    "        ############################################################\n",
    "        # Get features and labels\n",
    "        ############################################################\n",
    "        y = df_all_combined['label'].tolist()\n",
    "        X = df_all_combined.drop(columns=['label'])\n",
    "        \n",
    "        ############################################################\n",
    "        # Performance Store\n",
    "        ############################################################\n",
    "        list_cm = 0\n",
    "        list_roc_auc, list_pr_auc = [], []\n",
    "        list_pre, list_rec, list_f1 = [], [], []\n",
    "        list_acc, list_spe, list_sen = [], [], []\n",
    "        \n",
    "        ############################################################\n",
    "        # K-fold Cross Validation model evaluation\n",
    "        ############################################################\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_idx = 1\n",
    "        for train_ids, test_ids in kfold.split(X, y):\n",
    "            \n",
    "            ############################################################\n",
    "            # Get dataset split\n",
    "            ############################################################\n",
    "            df_train = X.loc[train_ids]\n",
    "            y_train = np.array(y)[train_ids]\n",
    "    \n",
    "            df_test = X.loc[test_ids]\n",
    "            y_test = np.array(y)[test_ids]\n",
    "    \n",
    "            ############################################################\n",
    "            # Drop useless columns\n",
    "            ############################################################\n",
    "            def drop_columns(df):\n",
    "                columns = [\n",
    "                    'dataset', 'filename', 'filepath', 'age', 'gender', 'status',\n",
    "                    'duration', 'duration_segment', 'sample_frequency', 'mean_amplitude',\n",
    "                    'segment_shape',\n",
    "                    ]\n",
    "                for col in columns:\n",
    "                    if col in df.columns:\n",
    "                        df = df.drop([col], axis=1)\n",
    "                \n",
    "                df = np.array(df)\n",
    "                return df\n",
    "    \n",
    "            X_train = drop_columns(df_train)\n",
    "            X_test = drop_columns(df_test)\n",
    "\n",
    "\n",
    "            ############################################################\n",
    "            # Scaling\n",
    "            ############################################################\n",
    "            scaler = StandardScaler()\n",
    "    \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "    \n",
    "            # Save the scaler to a file\n",
    "            scaler_filename = f\"{path_model_save}scaler_pipeline_CNN_{fold_idx}.joblib\"\n",
    "            joblib.dump(scaler, scaler_filename)\n",
    "    \n",
    "            ############################################################\n",
    "            # Oversampling (if required)\n",
    "            ############################################################\n",
    "            if datasets in [['fsdkaggle'], ['esc50']]:\n",
    "                try:\n",
    "                    oversample = SMOTE(sampling_strategy=0.5, k_neighbors=5)\n",
    "                    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "                except:\n",
    "                    oversample = SMOTE(sampling_strategy=0.5, k_neighbors=3)\n",
    "                    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "            ############################################################\n",
    "            # Reshaping\n",
    "            ############################################################\n",
    "            dimension_dictionary = {\n",
    "                1: 22,\n",
    "                5: 27,\n",
    "                10: 27,\n",
    "            }\n",
    "\n",
    "            dim_first = 128\n",
    "            input_shape = (dim_first, dimension_dictionary[window_length], 1)\n",
    "            X_train = X_train.reshape((len(X_train), dim_first,  dimension_dictionary[window_length]))\n",
    "            X_test = X_test.reshape((len(X_test), dim_first,  dimension_dictionary[window_length]))\n",
    "    \n",
    "            # Add one more axis for CNN\n",
    "            X_train = X_train[..., np.newaxis]\n",
    "            X_test = X_test[..., np.newaxis]\n",
    "            \n",
    "            ############################################################\n",
    "            # Create Model\n",
    "            ############################################################\n",
    "            model = get_CNN_model(input_shape)\n",
    "            # model.summary()\n",
    "    \n",
    "            batch_size = 16\n",
    "            early_stopping_patience = 10\n",
    "\n",
    "            # Add early stopping\n",
    "            my_callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=path_model_save + 'Checkpoints/model_{epoch:02d}_' + f'CNN_{fold_idx}.keras', \n",
    "                    save_freq='epoch', \n",
    "                    save_best_only=True\n",
    "                    ),\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\", \n",
    "                    patience=early_stopping_patience, \n",
    "                    restore_best_weights=True\n",
    "                    )\n",
    "            ]\n",
    "    \n",
    "            # Fit Model\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=my_callbacks,\n",
    "                validation_split=0.15,\n",
    "                verbose=0,\n",
    "                )\n",
    "    \n",
    "            history_loss_acc(history)\n",
    "    \n",
    "            test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "            print('Test Accuracy: ', round(test_acc, 3))\n",
    "    \n",
    "            predictions = model.predict(X_test)\n",
    "            y_predict = []\n",
    "            for i in range(len(predictions)):\n",
    "                predict = np.argmax(predictions[i])\n",
    "                y_predict.append(predict)\n",
    "                        \n",
    "            ############################################################\n",
    "            # Append predictions to df_test and save\n",
    "            ############################################################\n",
    "            df_test['true'] = y_test\n",
    "            df_test['pred'] = y_predict\n",
    "            \n",
    "            path_results_save = f'Results/Results CNN/{dataset_str}/CNN_{window_length}s/'\n",
    "            if not os.path.exists(path_results_save):\n",
    "                os.makedirs(path_results_save)\n",
    "            \n",
    "            df_test.to_csv(f'{path_results_save}Fold_{fold_idx}.csv', index=False)\n",
    "            \n",
    "            ############################################################\n",
    "            # Get evaluation metrics\n",
    "            ############################################################\n",
    "            acc = accuracy_score(y_test, y_predict)\n",
    "            cm = evaluate_matrix(y_test, y_predict)\n",
    "            roc_auc, pr_auc = ROC_PR_curve(y_test, predictions)\n",
    "            pre = precision_score(y_test, y_predict)\n",
    "            rec = recall_score(y_test, y_predict)\n",
    "            f1 = f1_score(y_test, y_predict)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "            spe = tn / (tn + fp)\n",
    "            sen = rec\n",
    "            \n",
    "            ############################################################\n",
    "            # Append results\n",
    "            ############################################################\n",
    "            list_acc.append(acc)\n",
    "            list_cm = list_cm + cm\n",
    "            list_roc_auc.append(roc_auc)\n",
    "            list_pr_auc.append(pr_auc)\n",
    "            list_pre.append(pre)\n",
    "            list_rec.append(rec)\n",
    "            list_f1.append(f1)\n",
    "            list_spe.append(spe)\n",
    "            list_sen.append(sen)\n",
    "\n",
    "            print(f\"Fold {fold_idx} - F1: {round(f1, 3)}\")\n",
    "            \n",
    "            ############################################################\n",
    "            # Save model\n",
    "            ############################################################\n",
    "            # Serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(f\"{path_model_save}model_{fold_idx}.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            \n",
    "            model.save_weights(f\"{path_model_save}model_{fold_idx}.weights.h5\")\n",
    "            model.save(f\"{path_model_save}model_{fold_idx}.h5\")\n",
    "            # loaded_model = get_model()\n",
    "            # loaded_model.load_weights('Results/Model/kfold.h5')\n",
    "    \n",
    "            ############################################################\n",
    "            # Save results\n",
    "            ############################################################\n",
    "            results = [\n",
    "                ', '.join(datasets), count_labels, \n",
    "                window_length, overlap,\n",
    "                'CNN', fold_idx,\n",
    "                acc, sen, spe, pre, rec, f1, roc_auc, pr_auc, cm]\n",
    "            df_results.append(results)\n",
    "    \n",
    "            # To the next fold\n",
    "            fold_idx = fold_idx + 1\n",
    "            \n",
    "        results = [\n",
    "            ', '.join(datasets),  count_labels,\n",
    "            window_length, overlap,\n",
    "            'CNN', 'Avg',\n",
    "            np.mean(list_acc),\n",
    "            np.mean(list_sen),\n",
    "            np.mean(list_spe),\n",
    "            np.mean(list_pre),\n",
    "            np.mean(list_rec),\n",
    "            np.mean(list_f1),\n",
    "            np.mean(list_roc_auc),\n",
    "            np.mean(list_pr_auc),\n",
    "            list_cm]\n",
    "        df_results.append(results)\n",
    "        \n",
    "        print(list_cm)\n",
    "        print(f'ROC AUC: {np.mean(list_roc_auc)}')\n",
    "        print(f'PR AUC: {np.mean(list_pr_auc)}')\n",
    "        print(f'F1: {np.mean(list_f1)}')\n",
    "    \n",
    "    columns = ['dataset', 'label_count', 'window_length', 'overlap',\n",
    "               'model', 'fold', \n",
    "               'acc', 'sen', 'spe', 'pre', 'rec', 'f1', 'auc', 'auprc', 'cm']    \n",
    "    df_results = pd.DataFrame(df_results, columns = columns)\n",
    "    df_results.to_csv(f'Results/Model_CNN/results_prediction_CNN_{window_length}s_{overlap}.csv', index=False)\n",
    "\n",
    "print('#'*60)\n",
    "print('DONE')\n",
    "print('#'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate\n",
    "for window_length in [1, 5, 10]:\n",
    "    df = pd.read_csv(f'Results/Model_CNN/results_prediction_CNN_{window_length}s_{overlap}.csv')\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Display or save the result\n",
    "print(combined_df)\n",
    "combined_df.to_csv(f'Results/Model_CNN/results_prediction_CNN_All.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
