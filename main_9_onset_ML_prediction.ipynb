{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 03:44:09.734402: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-18 03:44:09.925600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-18 03:44:10.028094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-18 03:44:10.035518: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-18 03:44:10.132332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-18 03:44:21.156983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    scale\n",
    "    )\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold\n",
    "    ) \n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score, \n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    "    )\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Imbalance learning\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from functions_model import get_NN_model, history_loss_acc, evaluate_matrix, ROC_PR_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to evaluate\n",
    "models_dict = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"NB\": GaussianNB(),\n",
    "    \"NN\": MLPClassifier(hidden_layer_sizes=(100,)),\n",
    "    \"GB\": GradientBoostingClassifier(),\n",
    "    \"Keras_NN\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = [\n",
    "    # ['fsdkaggle'],    # 2% cough Counter({0: 1570, 1: 30})\n",
    "    # # ['virufy'],       # 100% cough Counter({1: 121})\n",
    "    # ['esc50'],        # 2% cough Counter({0: 1960, 1: 40})\n",
    "    # ['coughvid'],     # 30% cough Counter({1: 19777, 0: 10267})\n",
    "    # ['coswara'],      # 25% cough Counter({0: 18914, 1: 5408})\n",
    "    ['coswara', 'coughvid', 'esc50', 'fsdkaggle', 'virufy'], \n",
    "      \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "coswara, coughvid, esc50, fsdkaggle, virufy\n",
      "Window length: 1\n",
      "############################################################\n",
      "{0: 1000, 1: 1000}\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - LR - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.812\n",
      "Fold 2 - F1: 0.82\n",
      "Fold 3 - F1: 0.786\n",
      "Fold 4 - F1: 0.819\n",
      "Fold 5 - F1: 0.837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       200\n",
      "           1       0.82      0.86      0.84       200\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n",
      "[[767 233]\n",
      " [152 848]]\n",
      "ROC AUC: 0.869\n",
      "PR AUC: 0.815\n",
      "F1: 0.815\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - DT - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.747\n",
      "Fold 2 - F1: 0.751\n",
      "Fold 3 - F1: 0.688\n",
      "Fold 4 - F1: 0.739\n",
      "Fold 5 - F1: 0.757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75       200\n",
      "           1       0.74      0.77      0.76       200\n",
      "\n",
      "    accuracy                           0.75       400\n",
      "   macro avg       0.75      0.75      0.75       400\n",
      "weighted avg       0.75      0.75      0.75       400\n",
      "\n",
      "[[729 271]\n",
      " [259 741]]\n",
      "ROC AUC: 0.735\n",
      "PR AUC: 0.802\n",
      "F1: 0.736\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - RF - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.799\n",
      "Fold 2 - F1: 0.811\n",
      "Fold 3 - F1: 0.799\n",
      "Fold 4 - F1: 0.833\n",
      "Fold 5 - F1: 0.832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       200\n",
      "           1       0.80      0.86      0.83       200\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.83      0.82      0.82       400\n",
      "weighted avg       0.83      0.82      0.82       400\n",
      "\n",
      "[[781 219]\n",
      " [162 838]]\n",
      "ROC AUC: 0.885\n",
      "PR AUC: 0.864\n",
      "F1: 0.815\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - SVM - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.834\n",
      "Fold 2 - F1: 0.832\n",
      "Fold 3 - F1: 0.805\n",
      "Fold 4 - F1: 0.832\n",
      "Fold 5 - F1: 0.849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       200\n",
      "           1       0.83      0.87      0.85       200\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.85      0.84      0.84       400\n",
      "weighted avg       0.85      0.84      0.84       400\n",
      "\n",
      "[[793 207]\n",
      " [143 857]]\n",
      "ROC AUC: 0.88\n",
      "PR AUC: 0.849\n",
      "F1: 0.83\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - KNN - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.785\n",
      "Fold 2 - F1: 0.779\n",
      "Fold 3 - F1: 0.752\n",
      "Fold 4 - F1: 0.763\n",
      "Fold 5 - F1: 0.787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78       200\n",
      "           1       0.78      0.80      0.79       200\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.79      0.79      0.78       400\n",
      "weighted avg       0.79      0.79      0.78       400\n",
      "\n",
      "[[717 283]\n",
      " [192 808]]\n",
      "ROC AUC: 0.825\n",
      "PR AUC: 0.832\n",
      "F1: 0.773\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - NB - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.798\n",
      "Fold 2 - F1: 0.814\n",
      "Fold 3 - F1: 0.794\n",
      "Fold 4 - F1: 0.796\n",
      "Fold 5 - F1: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.72      0.80       200\n",
      "           1       0.77      0.91      0.83       200\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.83      0.81      0.81       400\n",
      "weighted avg       0.83      0.81      0.81       400\n",
      "\n",
      "[[707 293]\n",
      " [126 874]]\n",
      "ROC AUC: 0.86\n",
      "PR AUC: 0.814\n",
      "F1: 0.807\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - NN - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.829\n",
      "Fold 2 - F1: 0.834\n",
      "Fold 3 - F1: 0.797\n",
      "Fold 4 - F1: 0.825\n",
      "Fold 5 - F1: 0.861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84       200\n",
      "           1       0.81      0.92      0.86       200\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.86      0.85      0.85       400\n",
      "weighted avg       0.86      0.85      0.85       400\n",
      "\n",
      "[[778 222]\n",
      " [134 866]]\n",
      "ROC AUC: 0.888\n",
      "PR AUC: 0.856\n",
      "F1: 0.829\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - GB - 1\n",
      "##################################################\n",
      "Fold 1 - F1: 0.819\n",
      "Fold 2 - F1: 0.809\n",
      "Fold 3 - F1: 0.776\n",
      "Fold 4 - F1: 0.832\n",
      "Fold 5 - F1: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.77      0.82       200\n",
      "           1       0.79      0.90      0.84       200\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.84      0.83      0.83       400\n",
      "weighted avg       0.84      0.83      0.83       400\n",
      "\n",
      "[[765 235]\n",
      " [150 850]]\n",
      "ROC AUC: 0.879\n",
      "PR AUC: 0.852\n",
      "F1: 0.815\n",
      "\n",
      "##################################################\n",
      "coswara coughvid esc50 fsdkaggle virufy - Keras_NN - 1\n",
      "##################################################\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8620 - loss: 0.6446  \n",
      "Test Accuracy:  0.863\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - F1: 0.859\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.7998 - loss: 0.6809\n",
      "Test Accuracy:  0.8\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - F1: 0.809\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - accuracy: 0.8194 - loss: 1.0191\n",
      "Test Accuracy:  0.837\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - F1: 0.853\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.8388 - loss: 0.7172\n",
      "Test Accuracy:  0.868\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - F1: 0.87\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.8595 - loss: 0.7019\n",
      "Test Accuracy:  0.87\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - F1: 0.874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.87       200\n",
      "           1       0.85      0.90      0.87       200\n",
      "\n",
      "    accuracy                           0.87       400\n",
      "   macro avg       0.87      0.87      0.87       400\n",
      "weighted avg       0.87      0.87      0.87       400\n",
      "\n",
      "[[813 187]\n",
      " [118 882]]\n",
      "ROC AUC: 0.922\n",
      "PR AUC: 0.905\n",
      "F1: 0.853\n",
      "############################################################\n",
      "DONE\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "for segment_length in [0.1, 0.2, 0.3, 0.5, 0.7, 1]:\n",
    "    df_results = []\n",
    "    for datasets in list_datasets:\n",
    "        datasets.sort()\n",
    "        print('')\n",
    "        print('#'*60)\n",
    "        print(', '.join(datasets))\n",
    "        print(f'Window length: {segment_length}')\n",
    "        print('#'*60)\n",
    "        \n",
    "        dataset_str = '_'.join(datasets)\n",
    "        \n",
    "        ############################################################\n",
    "        # Load data\n",
    "        ############################################################\n",
    "        df_all_combined = pd.DataFrame()\n",
    "        for dataset in datasets:    \n",
    "            # Load onset data\n",
    "            df = pd.read_csv(f'Results_Onset/Features/ML/data_extracted_{dataset}_{segment_length}s_onset_label.csv')\n",
    "            df_all_combined = pd.concat([df_all_combined, df], axis=0)\n",
    "        df_all_combined = df_all_combined.reset_index(drop=True)\n",
    "        df_all_combined = df_all_combined.fillna(0)\n",
    "        \n",
    "        ############################################################\n",
    "        # Get label distribution\n",
    "        ############################################################\n",
    "        df_all_combined = df_all_combined[df_all_combined['mean_amplitude'] > 0.005].reset_index(drop=True)\n",
    "        df_all_combined = df_all_combined.sample(frac=1).groupby('label_onset').head(1000).reset_index(drop=True)\n",
    "        list_labels = df_all_combined['label_onset'].tolist()\n",
    "        count_labels = dict(Counter(list_labels))\n",
    "        pprint(count_labels)\n",
    "        \n",
    "        ############################################################\n",
    "        # Get features and labels\n",
    "        ############################################################\n",
    "        y = df_all_combined['label_onset'].tolist()\n",
    "        X = df_all_combined.drop(columns=['label_onset'])\n",
    "        \n",
    "        ############################################################\n",
    "        # Loop different models\n",
    "        ############################################################\n",
    "        for model_name, model_selected in models_dict.items():\n",
    "\n",
    "            path_model_save = f'Results_Onset/Model_Onset/{dataset_str}/{model_name}_{segment_length}s/'\n",
    "            \n",
    "            if not os.path.exists(path_model_save):\n",
    "                os.makedirs(path_model_save)\n",
    "            \n",
    "            print('')\n",
    "            print('#'*50)\n",
    "            print(' '.join(datasets), '-', model_name, '-', segment_length)\n",
    "            print('#'*50)\n",
    "        \n",
    "            ############################################################\n",
    "            # Performance Store\n",
    "            ############################################################\n",
    "            list_cm = 0\n",
    "            list_roc_auc, list_pr_auc = [], []\n",
    "            list_pre, list_rec, list_f1 = [], [], []\n",
    "            list_acc, list_spe, list_sen = [], [], []\n",
    "            \n",
    "            ############################################################\n",
    "            # K-fold Cross Validation model evaluation\n",
    "            ############################################################\n",
    "            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            fold_idx = 1\n",
    "            for train_ids, test_ids in kfold.split(X, y):\n",
    "                ############################################################\n",
    "                # Get dataset split\n",
    "                ############################################################\n",
    "                df_train = X.loc[train_ids]\n",
    "                y_train = np.array(y)[train_ids]\n",
    "    \n",
    "                df_test = X.loc[test_ids]\n",
    "                y_test = np.array(y)[test_ids]\n",
    "    \n",
    "                ############################################################\n",
    "                # Drop useless columns\n",
    "                ############################################################\n",
    "                def drop_columns(df):\n",
    "                    columns = [\n",
    "                        'dataset', 'filename', 'filepath', 'age', 'gender', 'status',\n",
    "                        'duration', 'duration_segment', 'sample_frequency',\n",
    "                        'mean', 'variance', 'std_dev', 'skewness', 'kurtosis',\n",
    "                        'median', 'range_val', 'iqr', 'mean_amplitude',\n",
    "                        'label',\n",
    "                        ]\n",
    "                    for col in columns:\n",
    "                        if col in df.columns:\n",
    "                            df = df.drop([col], axis=1)\n",
    "    \n",
    "                    # print(df.columns)\n",
    "                    \n",
    "                    df = np.array(df)\n",
    "                    return df\n",
    "    \n",
    "                X_train = drop_columns(df_train)\n",
    "                X_test = drop_columns(df_test)\n",
    "    \n",
    "                ############################################################\n",
    "                # Scaling\n",
    "                ############################################################\n",
    "                scaler = StandardScaler()\n",
    "    \n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "    \n",
    "                # Save the scaler to a file\n",
    "                scaler_filename = f\"{path_model_save}scaler__{fold_idx}.joblib\"\n",
    "                joblib.dump(scaler, scaler_filename)\n",
    "    \n",
    "                ############################################################\n",
    "                # Oversampling (if required)\n",
    "                ############################################################\n",
    "                if datasets in [['fsdkaggle'], ['esc50']]:\n",
    "                    try:\n",
    "                        oversample = SMOTE(sampling_strategy=0.5, k_neighbors=5)\n",
    "                        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "                    except:\n",
    "                        oversample = SMOTE(sampling_strategy=0.5, k_neighbors=3)\n",
    "                        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "                        \n",
    "                ############################################################\n",
    "                # Create Model\n",
    "                ############################################################\n",
    "                if model_name != 'Keras_NN':\n",
    "                    feature_selection = SelectPercentile(score_func=f_classif, percentile=10)\n",
    "                    # Create a pipeline with feature selection, scaling, and the model\n",
    "                    model = Pipeline([\n",
    "                        ('feature_selection', feature_selection),\n",
    "                        # ('scaling', scaler),\n",
    "                        ('classification', model_selected)\n",
    "                    ])\n",
    "                    \n",
    "                    model.fit(X_train, y_train)\n",
    "    \n",
    "                    predictions = model.predict_proba(X_test)\n",
    "                    y_predict = []\n",
    "                    for i in range(len(predictions)):\n",
    "                        predict = np.argmax(predictions[i])\n",
    "                        y_predict.append(predict)\n",
    "            \n",
    "                else:\n",
    "                    model = get_NN_model(len(X_train[0]))\n",
    "                    # model.summary()\n",
    "    \n",
    "                    batch_size = 16\n",
    "                    early_stopping_patience = 10\n",
    "    \n",
    "                    # Add early stopping\n",
    "                    my_callbacks = [\n",
    "                        tf.keras.callbacks.ModelCheckpoint(\n",
    "                            filepath=path_model_save + 'Checkpoints/model_{epoch:02d}_' + f'{fold_idx}.keras', \n",
    "                            save_freq='epoch', \n",
    "                            save_best_only=True\n",
    "                            ),\n",
    "                        tf.keras.callbacks.EarlyStopping(\n",
    "                            monitor=\"val_loss\", \n",
    "                            patience=early_stopping_patience, \n",
    "                            restore_best_weights=True\n",
    "                            )\n",
    "                    ]\n",
    "    \n",
    "                    # Fit Model\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=my_callbacks,\n",
    "                        validation_split=0.15,\n",
    "                        verbose=0,\n",
    "                        )\n",
    "    \n",
    "                    history_loss_acc(history)\n",
    "            \n",
    "                    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "                    print('Test Accuracy: ', round(test_acc, 3))\n",
    "    \n",
    "                    predictions = model.predict(X_test)\n",
    "                    y_predict = []\n",
    "                    for i in range(len(predictions)):\n",
    "                        predict = np.argmax(predictions[i])\n",
    "                        y_predict.append(predict)\n",
    "                            \n",
    "                ############################################################\n",
    "                # Append predictions to df_test and save\n",
    "                ############################################################\n",
    "                df_test['true'] = y_test\n",
    "                df_test['pred'] = y_predict\n",
    "                \n",
    "                path_results_save = f'Results_Onset/Results_Onset/{dataset_str}/{model_name}_{segment_length}s/'\n",
    "                if not os.path.exists(path_results_save):\n",
    "                    os.makedirs(path_results_save)\n",
    "                \n",
    "                df_test.to_csv(f'{path_results_save}Fold_{fold_idx}.csv', index=False)\n",
    "                \n",
    "                ############################################################\n",
    "                # Get evaluation metrics\n",
    "                ############################################################\n",
    "                acc = accuracy_score(y_test, y_predict)\n",
    "                cm = evaluate_matrix(y_test, y_predict)\n",
    "                roc_auc, pr_auc = ROC_PR_curve(y_test, predictions)\n",
    "                pre = precision_score(y_test, y_predict)\n",
    "                rec = recall_score(y_test, y_predict)\n",
    "                f1 = f1_score(y_test, y_predict)\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "                spe = tn / (tn + fp)\n",
    "                sen = rec\n",
    "                \n",
    "                ############################################################\n",
    "                # Append results\n",
    "                ############################################################\n",
    "                list_acc.append(acc)\n",
    "                list_cm = list_cm + cm\n",
    "                list_roc_auc.append(roc_auc)\n",
    "                list_pr_auc.append(pr_auc)\n",
    "                list_pre.append(pre)\n",
    "                list_rec.append(rec)\n",
    "                list_f1.append(f1)\n",
    "                list_spe.append(spe)\n",
    "                list_sen.append(sen)\n",
    "    \n",
    "                print(f\"Fold {fold_idx} - F1: {round(f1, 3)}\")\n",
    "                \n",
    "                ############################################################\n",
    "                # Save model\n",
    "                ############################################################\n",
    "                if model_name == 'Keras_NN':\n",
    "                    # Serialize model to JSON\n",
    "                    model_json = model.to_json()\n",
    "                    with open(f\"{path_model_save}model_{fold_idx}.json\", \"w\") as json_file:\n",
    "                        json_file.write(model_json)\n",
    "                    \n",
    "                    model.save_weights(f\"{path_model_save}model_{fold_idx}.weights.h5\")\n",
    "                    model.save(f\"{path_model_save}model_{fold_idx}.h5\")\n",
    "                    # loaded_model = get_model()\n",
    "                    # loaded_model.load_weights('Results/Model/kfold.h5')\n",
    "                    \n",
    "                else:\n",
    "                    model_filename = f\"{path_model_save}model_{fold_idx}.joblib\"\n",
    "                    joblib.dump(model, model_filename)\n",
    "    \n",
    "                ############################################################\n",
    "                # Save results\n",
    "                ############################################################\n",
    "                results = [\n",
    "                    ', '.join(datasets), count_labels,\n",
    "                    segment_length,\n",
    "                    model_name, fold_idx,\n",
    "                    acc, sen, spe, pre, rec, f1, roc_auc, pr_auc, cm]\n",
    "                df_results.append(results)\n",
    "    \n",
    "                # To the next fold\n",
    "                fold_idx = fold_idx + 1\n",
    "                \n",
    "            results = [\n",
    "                ', '.join(datasets),  count_labels,\n",
    "                segment_length,\n",
    "                model_name, 'Avg',\n",
    "                np.mean(list_acc),\n",
    "                np.mean(list_sen),\n",
    "                np.mean(list_spe),\n",
    "                np.mean(list_pre),\n",
    "                np.mean(list_rec),\n",
    "                np.mean(list_f1),\n",
    "                np.mean(list_roc_auc),\n",
    "                np.mean(list_pr_auc),\n",
    "                list_cm]\n",
    "            df_results.append(results)\n",
    "    \n",
    "            print(classification_report(y_test, y_predict))\n",
    "            print(list_cm)\n",
    "            print(f'ROC AUC: {round(np.mean(list_roc_auc), 3)}')\n",
    "            print(f'PR AUC: {round(np.mean(list_pr_auc), 3)}')\n",
    "            print(f'F1: {round(np.mean(list_f1), 3)}')\n",
    "    \n",
    "    columns = ['dataset', 'label_count', 'window_length',\n",
    "               'model', 'fold', \n",
    "               'acc', 'sen', 'spe', 'pre', 'rec', 'f1', 'auc', 'auprc', 'cm']    \n",
    "    df_results = pd.DataFrame(df_results, columns = columns)\n",
    "    df_results.to_csv(f'Results_Onset/Model_Onset/results_prediction_{segment_length}s_onset.csv', index=False)\n",
    "\n",
    "print('#'*60)\n",
    "print('DONE')\n",
    "print('#'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         dataset         label_count  \\\n",
      "0    coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "1    coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "2    coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "3    coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "4    coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "..                                           ...                 ...   \n",
      "319  coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "320  coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "321  coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "322  coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "323  coswara, coughvid, esc50, fsdkaggle, virufy  {0: 1000, 1: 1000}   \n",
      "\n",
      "     window_length     model fold     acc    sen    spe       pre    rec  \\\n",
      "0              0.1        LR    1  0.7600  0.715  0.805  0.785714  0.715   \n",
      "1              0.1        LR    2  0.7900  0.805  0.775  0.781553  0.805   \n",
      "2              0.1        LR    3  0.7950  0.785  0.805  0.801020  0.785   \n",
      "3              0.1        LR    4  0.8025  0.765  0.840  0.827027  0.765   \n",
      "4              0.1        LR    5  0.7800  0.750  0.810  0.797872  0.750   \n",
      "..             ...       ...  ...     ...    ...    ...       ...    ...   \n",
      "319            1.0  Keras_NN    2  0.8000  0.845  0.755  0.775229  0.845   \n",
      "320            1.0  Keras_NN    3  0.8375  0.945  0.730  0.777778  0.945   \n",
      "321            1.0  Keras_NN    4  0.8675  0.885  0.850  0.855072  0.885   \n",
      "322            1.0  Keras_NN    5  0.8700  0.900  0.840  0.849057  0.900   \n",
      "323            1.0  Keras_NN  Avg  0.8475  0.882  0.813  0.828147  0.882   \n",
      "\n",
      "           f1       auc     auprc                       cm  \n",
      "0    0.748691  0.825475  0.821565  [[161  39]\\n [ 57 143]]  \n",
      "1    0.793103  0.839950  0.780418  [[155  45]\\n [ 39 161]]  \n",
      "2    0.792929  0.861450  0.860217  [[161  39]\\n [ 43 157]]  \n",
      "3    0.794805  0.862925  0.811860  [[168  32]\\n [ 47 153]]  \n",
      "4    0.773196  0.846650  0.846511  [[162  38]\\n [ 50 150]]  \n",
      "..        ...       ...       ...                      ...  \n",
      "319  0.808612  0.897325  0.868466  [[151  49]\\n [ 31 169]]  \n",
      "320  0.853273  0.911025  0.890909  [[146  54]\\n [ 11 189]]  \n",
      "321  0.869779  0.929725  0.910430  [[170  30]\\n [ 23 177]]  \n",
      "322  0.873786  0.936025  0.918995  [[168  32]\\n [ 20 180]]  \n",
      "323  0.852813  0.922280  0.904773  [[813 187]\\n [118 882]]  \n",
      "\n",
      "[324 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate\n",
    "for window_length in [0.1, 0.2, 0.3, 0.5, 0.7, 1]:\n",
    "    df = pd.read_csv(f'Results_Onset/Model_Onset/results_prediction_{window_length}s_onset.csv')\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Display or save the result\n",
    "print(combined_df)\n",
    "combined_df.to_csv(f'Results_Onset/Model_Onset/results_prediction_All_onset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
